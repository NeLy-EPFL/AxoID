{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI detection with Deep Learning\n",
    "Notebook used to test ROI detection using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time, shutil\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "from utils_common.image import imread_to_float, to_npint\n",
    "from utils_data import normalize_range, get_all_dataloaders, get_filenames\n",
    "from utils_loss import get_crop_loss, get_dice_metric, get_crop_dice_metric\n",
    "from utils_model import weights_initialization, CustomUNet\n",
    "from utils_train import train\n",
    "from utils_test import predict, predict_stack, evaluate, show_sample\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed*10 + 1234)\n",
    "torch.manual_seed(seed*100 + 4321)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Hyperparameters, folder names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "diceC_scale = 4.0 # scale of the cropping (w.r.t. ROI's bounding box) for the dice coef.\n",
    "synth_data = True\n",
    "synth_ratio = 0.5 # ratio of synthetic data vs. real data\n",
    "only_synth = True # If True, will use only the synthetic data (and all of it, at the opposite of ratio=1)\n",
    "\n",
    "input_channels = \"R\" # Channel to use as input\n",
    "u_depth = 4\n",
    "out1_channels = 16\n",
    "\n",
    "model_name = \"test\"\n",
    "model_dir = \"models/\"\n",
    "data_dir = \"../dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Create dataloaders, and build data-related constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71000 images of size 192x256.\n",
      "40000 to train (of synthetic data only).\n",
      "14100 to validation\n",
      "16900 to test\n",
      "120.000 positive weighting.\n"
     ]
    }
   ],
   "source": [
    "# Create transforms that pad images to multiple of 2 ** u_depth if needed\n",
    "def pad_transform(image):\n",
    "    factor = 2 ** u_depth\n",
    "    if image.ndim == 3:\n",
    "        height, width = image.shape[1:]\n",
    "    elif image.ndim == 2:\n",
    "        height, width = image.shape\n",
    "        \n",
    "    # Do nothing if image has correct shape\n",
    "    if height % factor == 0 and width % factor == 0:\n",
    "        return image\n",
    "    \n",
    "    height_pad = (factor - height % factor) * bool(height % factor)\n",
    "    width_pad = (factor - width % factor) * bool(width % factor)\n",
    "    padding = [(int(np.floor(height_pad/2)), int(np.ceil(height_pad/2))), \n",
    "               (int(np.floor(width_pad/2)), int(np.ceil(width_pad/2)))]\n",
    "    if image.ndim == 3:\n",
    "        return np.pad(image, [(0,0)] + padding, 'constant')\n",
    "    elif image.ndim == 2:\n",
    "        return np.pad(image, padding, 'constant')\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = get_all_dataloaders(\n",
    "    data_dir,\n",
    "    batch_size, \n",
    "    input_channels = input_channels,\n",
    "    test_dataloader = True,\n",
    "    synthetic_data = synth_data, synthetic_ratio = synth_ratio,\n",
    "    synthetic_only = only_synth,\n",
    "    train_transform = pad_transform, train_target_transform = pad_transform,\n",
    "    eval_transform = pad_transform, eval_target_transform = pad_transform\n",
    ")\n",
    "\n",
    "N_TRAIN = len(dataloaders[\"train\"].dataset)\n",
    "N_VALID = len(dataloaders[\"valid\"].dataset)\n",
    "N_TEST = len(dataloaders[\"test\"].dataset)\n",
    "HEIGHT, WIDTH = io.imread(get_filenames(os.path.join(data_dir, \"train/\"))[0][0]).shape[:2]\n",
    "print(\"%d images of size %dx%d.\" % (N_TRAIN + N_VALID + N_TEST, HEIGHT, WIDTH))\n",
    "print(\"%d to train\" % N_TRAIN, end=\"\")\n",
    "if only_synth:\n",
    "    print(\" (of synthetic data only).\")\n",
    "elif synth_data:\n",
    "    if synth_ratio is None:\n",
    "        print(\" (with synthetic data)\")\n",
    "    else:\n",
    "        print(\" (with %d%% of synthetic data)\" % (synth_ratio * 100))\n",
    "else:\n",
    "    print(\"\")\n",
    "print(\"%d to validation\" % N_VALID)\n",
    "print(\"%d to test\" % N_TEST)\n",
    "\n",
    "# Compute class weights (as pixel imbalance) takes ~90s for 78 stacks\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "# for _, batch_y in dataloaders[\"train\"]:\n",
    "#     pos_count += (batch_y == 1).sum().item()\n",
    "#     neg_count += (batch_y == 0).sum().item()\n",
    "# pos_weight = torch.tensor(neg_count / pos_count).to(device)\n",
    "# Following has been pre-computed\n",
    "pos_weight = torch.tensor(120.0).to(device)\n",
    "if pos_count != 0 and neg_count != 0:\n",
    "    print(\"{} ROI pixels, and {} background --> {:f} positive weighting.\".format(\n",
    "        pos_count, neg_count, pos_weight.item()))\n",
    "else:\n",
    "    print(\"{:.3f} positive weighting.\".format(pos_weight.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "Define model, loss, metrics, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomUNet(\n",
      "  (activation): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convs): ModuleList(\n",
      "    (0): UNetConv(\n",
      "      (activation): ReLU()\n",
      "      (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): UNetConv(\n",
      "      (activation): ReLU()\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): UNetConv(\n",
      "      (activation): ReLU()\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): UNetConv(\n",
      "      (activation): ReLU()\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (midconv): UNetConv(\n",
      "    (activation): ReLU()\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upconvs): ModuleList(\n",
      "    (0): UNetUpConv(\n",
      "      (activation): ReLU()\n",
      "      (upconv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): UNetUpConv(\n",
      "      (activation): ReLU()\n",
      "      (upconv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): UNetUpConv(\n",
      "      (activation): ReLU()\n",
      "      (upconv): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): UNetUpConv(\n",
      "      (activation): ReLU()\n",
      "      (upconv): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (outconv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CustomUNet(len(input_channels), u_depth=u_depth, \n",
    "                   out1_channels=out1_channels, batchnorm=True, device=device)\n",
    "print(model)\n",
    "\n",
    "# Save the \"architecture\" of the model by copy/pasting the class definition file\n",
    "os.makedirs(os.path.join(model_dir, model_name), exist_ok=True)\n",
    "shutil.copy(\"utils_model.py\", os.path.join(model_dir, model_name, \"utils_model_save.py\"))\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='elementwise_mean', pos_weight=pos_weight)\n",
    "crop_loss = get_crop_loss(loss_fn, scale=diceC_scale, device=device)\n",
    "\n",
    "dice_metric = get_dice_metric()\n",
    "diceC_metric = get_crop_dice_metric(scale=diceC_scale)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2  (Elapsed time: 0h 00min 00s)\n",
      "---------------------------------------\n",
      "Batch (over 1250): 1...125...250...375..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/user/talabot/ROI_detection/deep_learning/utils_data.py\", line 58, in __getitem__\n",
      "    image = imread_to_float(self.x_filenames[idx], scaling=255)\n",
      "  File \"/home/user/talabot/ROI_detection/deep_learning/utils_common/image.py\", line 16, in imread_to_float\n",
      "    stack = io.imread(filename)\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/skimage/io/_io.py\", line 62, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 214, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    image.getdata()[0]\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/PIL/Image.py\", line 1253, in getdata\n",
      "    self.load()\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/PIL/ImageFile.py\", line 221, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/PIL/PngImagePlugin.py\", line 619, in load_read\n",
      "    self.fp.read(4)  # CRC\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc4b5d95438>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/user/anaconda3/envs/talabot/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 32423) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3e02fa581fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                             \u001b[0mreplace_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                             verbose = 1)\n\u001b[0m",
      "\u001b[0;32m~/talabot/ROI_detection/deep_learning/utils_train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, loss_fn, optimizer, n_epochs, metrics, criterion_metric, model_dir, replace_dir, verbose)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y_cpu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;31m# Copy tensor to the model device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_y_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model, history = train(model,\n",
    "                            dataloaders,\n",
    "                            loss_fn,\n",
    "                            optimizer,\n",
    "                            n_epochs,\n",
    "                            metrics = {\"lossC%.1f\"%diceC_scale: crop_loss, \n",
    "                                       \"dice\": dice_metric, \"diC%.1f\"%diceC_scale: diceC_metric},\n",
    "                            criterion_metric = \"dice\",\n",
    "                            model_dir = os.path.join(model_dir, model_name),\n",
    "                            replace_dir = True,\n",
    "                            \n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Loss\\n(crop scale = %.1f)\" % diceC_scale)\n",
    "plt.plot(history[\"epoch\"], history[\"loss\"], color=\"C0\")\n",
    "plt.plot(history[\"epoch\"], history[\"lossC%.1f\" % diceC_scale], \"--\", color=\"C0\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_loss\"], color=\"C1\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_lossC%.1f\" % diceC_scale], \"--\", color=\"C1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"train loss\", \"train cropped loss\", \"valid loss\", \"valid cropped loss\"])\n",
    "plt.subplot(122)\n",
    "plt.title(\"Dice coefficients\\n(crop scale = %.1f)\" % diceC_scale)\n",
    "plt.plot(history[\"epoch\"], history[\"dice\"], color=\"C0\")\n",
    "plt.plot(history[\"epoch\"], history[\"diC%.1f\" % diceC_scale], \"--\", color=\"C0\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_dice\"], color=\"C1\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_diC%.1f\" % diceC_scale], \"--\", color=\"C1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Dice coef.\")\n",
    "plt.ylim(0,1)\n",
    "plt.legend([\"train dice\", \"train cropped dice\", \"valid dice\", \"valid cropped dice\"])\n",
    "# plt.subplot(133)\n",
    "# plt.title(\"Cropped Dice coefficient (scale = %.1f)\" % diceC_scale)\n",
    "# plt.plot(history[\"epoch\"], history[\"diC%.1f\"%diceC_scale])\n",
    "# plt.plot(history[\"epoch\"], history[\"val_diC%.1f\"%diceC_scale])\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Cropped Dice coef.\")\n",
    "# plt.ylim(0,1)\n",
    "# plt.legend([\"train diC%.1f\"%diceC_scale, \"valid diC%.1f\"%diceC_scale])\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Test and visualize best model on valid/test data. Note that this is for debugging purposes: you should use the test data only at the very end! Otherwise, just use validation data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load best model /!\\ Always check which class to import !\n",
    "name_test = \"unet4-16_synth_only_2-4neur\"\n",
    "sys.path.append(os.path.join(model_dir, name_test))\n",
    "\n",
    "from utils_model_save import CustomUNet as ModelNet\n",
    "\n",
    "best_model = ModelNet(len(input_channels), u_depth=u_depth, \n",
    "                      out1_channels=out1_channels, batchnorm=True, device=device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(model_dir, name_test, \"model_best.pth\")))\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test over test data\n",
    "test_metrics = evaluate(best_model, dataloaders[\"test\"], \n",
    "                        {\"loss\": loss_fn, \"dice\": dice_metric, \"diC%.1f\"%diceC_scale: diceC_metric})\n",
    "print(\"Test loss = {}\\nDice coef = {}\\nDice C{:.1f} = {}\".format(\n",
    "    test_metrics[\"loss\"], test_metrics[\"dice\"], diceC_scale, test_metrics[\"diC%.1f\"%diceC_scale]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(best_model, dataloaders[\"valid\"], n_samples=4, \n",
    "            post_processing = lambda preds: torch.tensor(torch.sigmoid(preds) > 0.5, dtype=torch.float32), \n",
    "            metrics = ({\"lossC%.1f\"%diceC_scale: crop_loss, \n",
    "                        \"dice\": dice_metric, \"diC%.1f\"%diceC_scale: diceC_metric}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(best_model, dataloaders[\"valid\"], n_samples=4, \n",
    "            post_processing = None, \n",
    "            metrics = ({\"lossC%.1f\"%diceC_scale: crop_loss, \n",
    "                        \"dice\": dice_metric, \"diC%.1f\"%diceC_scale: diceC_metric}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a full stack with no ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rgb_stack = imread_to_float(\n",
    "    \"/home/user/talabot/workdir/20180716_R85A02-tdTomGC6fopt-fly4/R85A02-tdTomGC6fopt-fly4-001/RGB.tif\")\n",
    "channels = {\"R\": rgb_stack[...,0], \"G\": rgb_stack[...,1], \"B\": rgb_stack[...,2]}\n",
    "stack = np.stack([channels[channel] for channel in input_channels], axis=1)\n",
    "\n",
    "start_predict = time.time()\n",
    "predictions = predict_stack(best_model, stack, batch_size, input_channels=input_channels)\n",
    "print(\"Prediction took %.1fs.\" % (time.time() - start_predict))\n",
    "\n",
    "@interact(num = widgets.BoundedIntText(min=0, max=len(rgb_stack) - 1, description='image:'))\n",
    "def plot_stack_img(num=0):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(221)\n",
    "    plt.title(\"RGB data\")\n",
    "    plt.imshow(rgb_stack[num])\n",
    "    plt.subplot(222)\n",
    "    plt.title(\"Input\")\n",
    "    if input_channels == \"R\":\n",
    "        plt.imshow(rgb_stack[num,...,0], vmin=0, vmax=1, cmap=\"gray\")\n",
    "    elif input_channels == \"RG\":\n",
    "        plt.imshow(rgb_stack[num], vmin=0, vmax=1)\n",
    "    plt.subplot(223)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(predictions[num], vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.subplot(224)\n",
    "    plt.title(\"Binary reduction\")\n",
    "    plt.imshow(predictions[num] > 0.5, vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
