{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI detection with Deep Learning\n",
    "Notebook used to test ROI detection using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time, shutil\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "from utils_common.image import imread_to_float, to_npint, overlay_mask\n",
    "from utils_data import normalize_range, get_all_dataloaders, get_filenames\n",
    "from utils_loss import get_crop_loss, get_dice_metric, get_crop_dice_metric\n",
    "from utils_model import weights_initialization, CustomUNet\n",
    "from utils_train import train\n",
    "from utils_test import predict, predict_stack, evaluate, show_sample\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed*10 + 1234)\n",
    "torch.manual_seed(seed*100 + 4321)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Hyperparameters, folder names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "diceC_scale = 4.0 # scale of the cropping (w.r.t. ROI's bounding box) for the dice coef.\n",
    "\n",
    "synth_data = False\n",
    "synth_ratio = 0.5 # ratio of synthetic data vs. real data\n",
    "only_synth = False # If True, will use only the synthetic data (and all of it, at the opposite of ratio=1)\n",
    "\n",
    "use_masks = False # If True, will use masks for the loss\n",
    "data_aug = True # If True, will use data augmentation (see below for augmentation sequence)\n",
    "\n",
    "input_channels = \"R\" # Channel to use as input\n",
    "u_depth = 4\n",
    "out1_channels = 16\n",
    "\n",
    "model_name = \"test\"\n",
    "model_dir = \"models/\"\n",
    "data_dir = \"/data/talabot/dataset_cv-annotated/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Create dataloaders, and build data-related constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation enabled.\n",
      "28800 images:\n",
      "18000 to train\n",
      "6000 to validation\n",
      "4800 to test\n",
      "11126882 ROI pixels, and 873609118 background --> 78.513382 positive weighting.\n"
     ]
    }
   ],
   "source": [
    "# Create random augment sequence for data augmentation\n",
    "if data_aug:\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GammaContrast((0.7, 1.3)) # Gamma correction\n",
    "    ])\n",
    "    aug_fn = seq.augment_image\n",
    "    print(\"Data augmentation is enabled.\")\n",
    "else:\n",
    "    aug_fn = lambda x: x # identity function\n",
    "\n",
    "# Create transforms that pad images to multiple of 2 ** u_depth if needed\n",
    "def pad_transform(image):\n",
    "    factor = 2 ** u_depth\n",
    "    if image.ndim == 3:\n",
    "        height, width = image.shape[1:]\n",
    "    elif image.ndim == 2:\n",
    "        height, width = image.shape\n",
    "        \n",
    "    # Do nothing if image has correct shape\n",
    "    if height % factor == 0 and width % factor == 0:\n",
    "        return image\n",
    "    \n",
    "    height_pad = (factor - height % factor) * bool(height % factor)\n",
    "    width_pad = (factor - width % factor) * bool(width % factor)\n",
    "    padding = [(int(np.floor(height_pad/2)), int(np.ceil(height_pad/2))), \n",
    "               (int(np.floor(width_pad/2)), int(np.ceil(width_pad/2)))]\n",
    "    if image.ndim == 3:\n",
    "        return np.pad(image, [(0,0)] + padding, 'constant')\n",
    "    elif image.ndim == 2:\n",
    "        return np.pad(image, padding, 'constant')\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = get_all_dataloaders(\n",
    "    data_dir,\n",
    "    batch_size, \n",
    "    input_channels = input_channels,\n",
    "    test_dataloader = True,\n",
    "    synthetic_data = synth_data, synthetic_ratio = synth_ratio,\n",
    "    synthetic_only = only_synth, use_masks=use_masks,\n",
    "    train_transform = lambda img: normalize_range(pad_transform(aug_fn(img))), # With possible augmentation\n",
    "    train_target_transform = pad_transform,\n",
    "    eval_transform = lambda img: normalize_range(pad_transform(img)), eval_target_transform = pad_transform\n",
    ")\n",
    "\n",
    "N_TRAIN = len(dataloaders[\"train\"].dataset)\n",
    "N_VALID = len(dataloaders[\"valid\"].dataset)\n",
    "N_TEST = len(dataloaders[\"test\"].dataset)\n",
    "print(\"%d images:\" % (N_TRAIN + N_VALID + N_TEST))\n",
    "print(\"%d to train\" % N_TRAIN, end=\"\")\n",
    "if only_synth:\n",
    "    print(\" (of synthetic data only).\")\n",
    "elif synth_data:\n",
    "    if synth_ratio is None:\n",
    "        print(\" (with synthetic data)\")\n",
    "    else:\n",
    "        print(\" (with %d%% of synthetic data)\" % (synth_ratio * 100))\n",
    "else:\n",
    "    print(\"\")\n",
    "print(\"%d to validation\" % N_VALID)\n",
    "print(\"%d to test\" % N_TEST)\n",
    "if use_masks:\n",
    "    print(\"Use of masks is enabled.\")\n",
    "\n",
    "# Compute class weights (as pixel imbalance) takes ~90s for 78 stacks\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "for filename in dataloaders[\"train\"].dataset.y_filenames:\n",
    "    y = io.imread(filename)\n",
    "    pos_count += (y == 255).sum()\n",
    "    neg_count += (y == 0).sum()\n",
    "pos_weight = torch.tensor(neg_count / pos_count).to(device)\n",
    "# Following has been pre-computed\n",
    "# pos_weight = torch.tensor(120.0).to(device)\n",
    "if pos_count != 0 and neg_count != 0:\n",
    "    print(\"{} ROI pixels, and {} background --> {:f} positive weighting.\".format(\n",
    "        pos_count, neg_count, pos_weight.item()))\n",
    "else:\n",
    "    print(\"{:.3f} positive weighting (pre-computed).\".format(pos_weight.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "Define model, loss, metrics, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomUNet(\n",
      "  (activation): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convs): ModuleList(\n",
      "    (0): UNetConv(\n",
      "      (activation): ReLU()\n",
      "      (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): UNetConv(\n",
      "      (activation): ReLU()\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): UNetConv(\n",
      "      (activation): ReLU()\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): UNetConv(\n",
      "      (activation): ReLU()\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (midconv): UNetConv(\n",
      "    (activation): ReLU()\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upconvs): ModuleList(\n",
      "    (0): UNetUpConv(\n",
      "      (activation): ReLU()\n",
      "      (upconv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): UNetUpConv(\n",
      "      (activation): ReLU()\n",
      "      (upconv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): UNetUpConv(\n",
      "      (activation): ReLU()\n",
      "      (upconv): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): UNetUpConv(\n",
      "      (activation): ReLU()\n",
      "      (upconv): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (outconv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CustomUNet(len(input_channels), u_depth=u_depth, \n",
    "                   out1_channels=out1_channels, batchnorm=True, device=device)\n",
    "print(model)\n",
    "\n",
    "# Save the \"architecture\" of the model by copy/pasting the class definition file\n",
    "os.makedirs(os.path.join(model_dir, model_name), exist_ok=True)\n",
    "shutil.copy(\"utils_model.py\", os.path.join(model_dir, model_name, \"utils_model_save.py\"))\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='elementwise_mean', pos_weight=pos_weight)\n",
    "crop_loss = get_crop_loss(loss_fn, scale=diceC_scale, device=device)\n",
    "\n",
    "dice_metric = get_dice_metric()\n",
    "diceC_metric = get_crop_dice_metric(scale=diceC_scale)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2  (Elapsed time: 0h 00min 00s)\n",
      "---------------------------------------\n",
      "Batch (over 563): 1...56...112...168...224...280...336...392...448...504...560...\n",
      "Train loss: 0.223184 - lossC4.0: 0.592509 - dice: 0.644613 - diC4.0: 0.645154\n",
      "Valid loss: 0.122367 - lossC4.0: 0.598438 - dice: 0.731286 - diC4.0: 0.688668\n",
      "\n",
      "Epoch 2/2  (Elapsed time: 0h 04min 50s)\n",
      "---------------------------------------\n",
      "Batch (over 563): 1...56...112...168...224...280..."
     ]
    }
   ],
   "source": [
    "best_model, history = train(model,\n",
    "                            dataloaders,\n",
    "                            loss_fn,\n",
    "                            optimizer,\n",
    "                            n_epochs,\n",
    "                            metrics = {\"lossC%.1f\"%diceC_scale: crop_loss, \n",
    "                                       \"dice\": dice_metric, \"diC%.1f\"%diceC_scale: diceC_metric},\n",
    "                            criterion_metric = \"dice\",\n",
    "                            model_dir = os.path.join(model_dir, model_name),\n",
    "                            replace_dir = True,\n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Loss\\n(crop scale = %.1f)\" % diceC_scale)\n",
    "plt.plot(history[\"epoch\"], history[\"loss\"], color=\"C0\")\n",
    "plt.plot(history[\"epoch\"], history[\"lossC%.1f\" % diceC_scale], \"--\", color=\"C0\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_loss\"], color=\"C1\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_lossC%.1f\" % diceC_scale], \"--\", color=\"C1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"train loss\", \"train cropped loss\", \"valid loss\", \"valid cropped loss\"])\n",
    "plt.subplot(122)\n",
    "plt.title(\"Dice coefficients\\n(crop scale = %.1f)\" % diceC_scale)\n",
    "plt.plot(history[\"epoch\"], history[\"dice\"], color=\"C0\")\n",
    "plt.plot(history[\"epoch\"], history[\"diC%.1f\" % diceC_scale], \"--\", color=\"C0\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_dice\"], color=\"C1\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_diC%.1f\" % diceC_scale], \"--\", color=\"C1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Dice coef.\")\n",
    "plt.ylim(0,1)\n",
    "plt.legend([\"train dice\", \"train cropped dice\", \"valid dice\", \"valid cropped dice\"])\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Test and visualize best model on valid/test data. Note that this is for debugging purposes: you should use the test data only at the very end! Otherwise, just use validation data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load best model /!\\ Always check which class to import !\n",
    "name_test = \"unet4-16_R_cv-annotated\"\n",
    "# name_test = \"unet4-16_synth_2-6neur_181205\"\n",
    "sys.path.append(os.path.join(model_dir, name_test))\n",
    "\n",
    "from utils_model_save import CustomUNet as ModelNet\n",
    "\n",
    "best_model = ModelNet(len(input_channels), u_depth=u_depth, \n",
    "                      out1_channels=out1_channels, batchnorm=True, device=device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(model_dir, name_test, \"model_best.pth\")))\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test over test data\n",
    "test_metrics = evaluate(best_model, dataloaders[\"valid\"], \n",
    "                        {\"loss\": lambda x,y,z: loss_fn(x[z], y[z]), \n",
    "                         \"lossC%.1f\"%diceC_scale: crop_loss, \n",
    "                        \"dice\": dice_metric, \"diC%.1f\"%diceC_scale: diceC_metric})\n",
    "print(\"Test loss = {}\\nDice coef = {}\\nDice C{:.1f} = {}\".format(\n",
    "    test_metrics[\"loss\"], test_metrics[\"dice\"], diceC_scale, test_metrics[\"diC%.1f\"%diceC_scale]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_sample(best_model, dataloaders[\"valid\"], n_samples=4, \n",
    "            post_processing = lambda preds: torch.tensor(torch.sigmoid(preds) > 0.5, dtype=torch.float32), \n",
    "            metrics = ({\"lossC%.1f\"%diceC_scale: crop_loss, \n",
    "                        \"dice\": dice_metric, \"diC%.1f\"%diceC_scale: diceC_metric}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(best_model, dataloaders[\"valid\"], n_samples=4, \n",
    "            post_processing = None, \n",
    "            metrics = ({\"lossC%.1f\"%diceC_scale: crop_loss, \n",
    "                        \"dice\": dice_metric, \"diC%.1f\"%diceC_scale: diceC_metric}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a full stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_name = \"/home/user/talabot/workdir/SS25451_20180824_fly1_007/\"\n",
    "# exp_name = \"/data/talabot/dataset_cv-annotated/validation_fail/A1_S47-3_fly1_S47_tdTom-GC6s_fly3_013/\"\n",
    "rgb_stack = imread_to_float(os.path.join(exp_name, \"RGB.tif\"))\n",
    "try:\n",
    "    seg_stack = imread_to_float(os.path.join(exp_name, \"seg_ROI.tif\"))\n",
    "except FileNotFoundError:\n",
    "    seg_stack = None\n",
    "channels = {\"R\": rgb_stack[...,0], \"G\": rgb_stack[...,1], \"B\": rgb_stack[...,2]}\n",
    "stack = np.stack([channels[channel] for channel in input_channels], axis=1)\n",
    "if use_masks:\n",
    "    try:\n",
    "        mask_stack = imread_to_float(os.path.join(exp_name, \"mask.tif\"))\n",
    "    except FileNotFoundError:\n",
    "        mask_stack = np.zeros_like(seg_stack)\n",
    "else:\n",
    "    mask_stack = np.zeros_like(seg_stack)\n",
    "mask_stack = 1 - mask_stack\n",
    "\n",
    "### Optional normalization ###\n",
    "stack = normalize_range(stack)\n",
    "\n",
    "start_predict = time.time()\n",
    "predictions = predict_stack(best_model, stack, batch_size, input_channels=input_channels)\n",
    "print(\"Prediction took %.1fs.\" % (time.time() - start_predict))\n",
    "\n",
    "if seg_stack is not None:\n",
    "    print(\"dice = %.3f\" % dice_metric(predictions[:len(seg_stack)], torch.from_numpy(seg_stack),\n",
    "                                      torch.from_numpy(mask_stack[:len(seg_stack)])))\n",
    "    print(\"cropped dice = %.3f\" % diceC_metric(predictions[:len(seg_stack)], torch.from_numpy(seg_stack),\n",
    "                                      torch.from_numpy(mask_stack[:len(seg_stack)])))\n",
    "\n",
    "predictions = torch.sigmoid(predictions)\n",
    "@interact(num = widgets.IntSlider(min=0, max=len(rgb_stack) - 1, description='image:'))\n",
    "def plot_stack_img(num=0):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(221)\n",
    "    plt.title(\"RGB data\")\n",
    "    plt.imshow(rgb_stack[num])\n",
    "    plt.subplot(222)\n",
    "    plt.title(\"Input\")\n",
    "    if input_channels == \"R\":\n",
    "        plt.imshow(rgb_stack[num,...,0], vmin=0, vmax=1, cmap=\"gray\")\n",
    "    elif input_channels == \"RG\":\n",
    "        plt.imshow(rgb_stack[num], vmin=0, vmax=1)\n",
    "    plt.subplot(223)\n",
    "    plt.title(\"Prediction (max = %.1f)\" % predictions[num].max())\n",
    "    plt.imshow(predictions[num], vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.subplot(224)\n",
    "    plt.title(\"Binary reduction\")\n",
    "    plt.imshow(predictions[num] > 0.5, vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if seg_stack is not None and num < len(seg_stack):\n",
    "        cmap = matplotlib.cm.get_cmap(\"viridis\")\n",
    "        cmap.set_under([1,0,0])\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(121)\n",
    "        plt.title(\"Target\")\n",
    "        plt.imshow(overlay_mask(seg_stack[num], 1-mask_stack[num]), cmap=\"gray\")\n",
    "        plt.subplot(122)\n",
    "        plt.title(\"Overlap\")\n",
    "        plt.imshow(2 * (predictions[num] > 0.5).cpu().numpy() - seg_stack[num], vmin=0, cmap=cmap)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "io.imsave(\"/home/user/talabot/out.tif\", to_npint(predictions.cpu().numpy() > 0.5)[:10])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from utils_common.image import overlay_mask_stack\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"gray\")\n",
    "cmap.set_under([1,0,0])\n",
    "cmap.set_over([0,1,0])\n",
    "\n",
    "pred = (predictions.cpu().numpy() > 0.5).astype(np.float)\n",
    "seg = seg_stack.astype(np.bool)\n",
    "mask = 1 - mask_stack\n",
    "\n",
    "correct = overlay_mask_stack(pred, np.logical_and(pred, seg), opacity=1, mask_color=[0,1,0])\n",
    "incorrect = overlay_mask_stack(correct, np.logical_and(seg, np.logical_not(pred)), opacity=1, mask_color=[1,0,0])\n",
    "final = overlay_mask_stack(incorrect, mask, opacity=0.5, mask_color=[1,1,0])\n",
    "\n",
    "@interact(num = (0, len(seg) - 1))\n",
    "def plot(num=0):\n",
    "    plt.imshow(final[num])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "io.imsave(\"/home/user/talabot/out.tif\", to_npint(final[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
