{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic generation\n",
    "Notebook used to make whole stacks of synthetic data based on single step results (synthetic_tests.ipynb).  \n",
    "\n",
    "**Note that there are a lot of hard coded numbers below. They are/have been empirically tuned by visual comparison to real data.**  \n",
    "**Note:** the script `run_generation.py` is used to create the stacks. Functions are then written both here and in the script. The script should be the latest working version, whereas here should be for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, time, pickle\n",
    "import warnings\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, draw, color, exposure, measure\n",
    "import skimage.morphology as morph\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import ndimage as ndi\n",
    "from scipy import signal\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from utils_common.image import to_npint, gray2red\n",
    "from utils_common.processing import flood_fill\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following are pre-computed on real data (corresponding to low laser gain). See stats_###.pkl & README.md.\n",
    "_BKG_MEAN_R = 0.04061809239988313 # mean value of red background (190222)\n",
    "_BKG_MEAN_G = 0.03090710807146899 # mean value of red background (190222)\n",
    "_BKG_STD = 0.005 # Standard deviation of mean value of background (empirically tuned)\n",
    "_ROI_MAX_1 = 0.2276730082246407 # fraction of red ROI with 1 as max intensity (181121)\n",
    "_ROI_MAX_MEAN = 0.6625502112855037 # mean of red ROI max (excluding 1.0) (181121)\n",
    "_ROI_MAX_STD = 0.13925117610178622 # std of red ROI max (excluding 1.0) (181121)\n",
    "\n",
    "# Threshold for the laser gain where saturation starts to occur\n",
    "_GAIN_T = 0.5\n",
    "\n",
    "# Following are the pre-generated GCaMP response kernel\n",
    "with open(\"GCaMP_kernel.pkl\", \"rb\") as f:\n",
    "    kernel_f, kernel_s = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one\n",
    "Function to create a synthetic image/stack from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neurons(n_neurons, shape, return_label):\n",
    "    \"\"\"Return gaussian and segmentation images corresponding to neurons.\"\"\"\n",
    "    ellipse_size = 1.5 # factor for the ground truth ellipse (normalized by std)\n",
    "    # Meshgrid for the gaussian weights\n",
    "    rows, cols = np.arange(shape[0]), np.arange(shape[1])\n",
    "    meshgrid = np.zeros(shape + (2,))\n",
    "    meshgrid[:,:,0], meshgrid[:,:,1] = np.meshgrid(cols, rows) # note the order\n",
    "    \n",
    "    gaussians = np.zeros((n_neurons,) + shape)\n",
    "    if return_label:\n",
    "        neuron_segs = np.zeros((n_neurons,) + shape, dtype=np.uint8)\n",
    "    else:\n",
    "        neuron_segs = np.zeros((n_neurons,) + shape, dtype=np.bool)\n",
    "    for i in range(n_neurons):\n",
    "        # Loop until the randomly generated neuron is in the image \n",
    "        # and doesn't overlap with another (can theoretically loop to infinity if too many neurons)\n",
    "        while True:\n",
    "            # Mean and covariance matrix of gaussian (empirically tuned)\n",
    "            # Note that x and y axes correspond to col and row \n",
    "            mean = np.array([np.random.randint(shape[1]), np.random.randint(shape[0])])\n",
    "            scale_x = shape[1] / 64\n",
    "            scale_y = shape[0] / 64\n",
    "            cross_corr = np.random.randint(-2, 3) * min(scale_x, scale_y)\n",
    "            cov = np.array([[np.random.randint(1, 4) * scale_x, cross_corr],\n",
    "                            [cross_corr, np.random.randint(10, 40) * scale_y]])\n",
    "\n",
    "            # Bounding ellipse\n",
    "            val, vec = np.linalg.eig(cov)\n",
    "            rotation = math.atan2(vec[0, np.argmax(val)], vec[1, np.argmax(val)])\n",
    "            rr, cc = draw.ellipse(mean[1], mean[0], \n",
    "                                  ellipse_size * np.sqrt(val[1]), \n",
    "                                  ellipse_size * np.sqrt(val[0]),\n",
    "                                  rotation=rotation)\n",
    "            # Check if outside the image\n",
    "            if (rr < 0).any() or (rr >= shape[0]).any() or (cc < 0).any() or (cc >= shape[1]).any():\n",
    "                continue\n",
    "            else:\n",
    "                # Check if overlapping/touching with any existing neuron\n",
    "                tmp_mask = np.zeros(shape, dtype=np.bool)\n",
    "                tmp_mask[rr,cc] = 1\n",
    "                if (neuron_segs[i, morph.dilation(tmp_mask)] != 0).any():\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "        neuron_segs[i, rr, cc] = 1 + i * return_label\n",
    "        \n",
    "        # Create gaussian weight image\n",
    "        gaussians[i,:,:] = multivariate_normal.pdf(meshgrid, mean, cov)\n",
    "        gaussians[i,:,:] /= gaussians[i,:,:].sum()\n",
    "    \n",
    "    return gaussians, neuron_segs\n",
    "\n",
    "\n",
    "def get_flurophores(n_neurons, n_images, gcamp_type):\n",
    "    \"\"\"Return the tdTomato level, and GCaMP dynamics.\"\"\"\n",
    "    fps = 2.4 # synthetic frame per seconds\n",
    "    # Choose which fluorophores are expressed for each neuron\n",
    "    c_presence = np.array([[True, True], [True, False], [False, True]], dtype=np.bool)\n",
    "    channel_neurons = c_presence[np.random.choice(len(c_presence), size=n_neurons, p=[0.9, 0.05, 0.05]), :]\n",
    "    \n",
    "    # tdTomato: choose max intensity of the neurons\n",
    "    tdTom_max = np.zeros(n_neurons)  \n",
    "    for i in range(n_neurons):\n",
    "        if channel_neurons[i, 0] == False:\n",
    "            continue\n",
    "        # Sample randomly the neuron maximum \n",
    "        if np.random.rand() < _ROI_MAX_1:\n",
    "            tdTom_max[i] = 1.0\n",
    "        else:\n",
    "            loc = _ROI_MAX_MEAN\n",
    "            scale = _ROI_MAX_STD\n",
    "            tdTom_max[i] = np.clip(np.random.normal(loc=loc, scale=scale), 0, 1)\n",
    "    \n",
    "    # GCaMP: create dynamics through time of the neurons\n",
    "    gcamp_dynamics = np.zeros((n_neurons, n_images))\n",
    "    # GCaMP type (50% 6f and 50% 6s)\n",
    "    if gcamp_type == \"6f\": # GCaMP6f\n",
    "        kernel_gcamp = kernel_f\n",
    "    elif gcamp_type == \"6s\": # GCaMP6s\n",
    "        kernel_gcamp = kernel_s\n",
    "    else:\n",
    "        raise ValueError(\"Unknown GCaMP type '{}'.\".format(gcamp_type))\n",
    "    t = np.arange(np.ceil(n_images / fps) * 1000) / 1000 # timesteps in ms\n",
    "    for i in range(n_neurons):\n",
    "        if channel_neurons[i, 1] == False:\n",
    "            continue\n",
    "        # Rate of firing\n",
    "        rate = np.zeros(len(t))\n",
    "        rate[np.random.randint(len(t), size=n_images // 10)] = 0.5\n",
    "        rate = np.convolve(rate, signal.gaussian(5000, 1000), mode='full')[:len(rate)].clip(0,1)\n",
    "        # Spiking (80%) or non-spiking (20%)\n",
    "        if np.random.rand() < 0.8:\n",
    "            spikes = np.random.poisson(rate / 250, size=len(t)).clip(0,1)\n",
    "            dynamics = np.convolve(spikes, kernel_gcamp, mode=\"full\")[:len(spikes)]\n",
    "        else:\n",
    "            dynamics = np.convolve(rate / 100, kernel_gcamp, mode=\"full\")[:len(rate)]\n",
    "        # Sub-sample to fps\n",
    "        gcamp_dynamics[i] = dynamics[::int(np.rint(1000/fps))][:n_images]\n",
    "        # If no red, assures a minimum to avoid invisible neurons\n",
    "        if channel_neurons[i,0]:\n",
    "            gcamp_dynamics[i] = gcamp_dynamics[i].clip(0,1)\n",
    "        else:\n",
    "            gcamp_dynamics[i] = gcamp_dynamics[i].clip(np.random.normal(0.4, 0.02),1)\n",
    "            \n",
    "    return tdTom_max, gcamp_dynamics\n",
    "\n",
    "\n",
    "def deform_neurons(n_neurons, shape, gaussians, neuron_segs):\n",
    "    \"\"\"Deform neurons and segmentation like the real acquisition system.\"\"\"\n",
    "    # For warping (like acquisition process):\n",
    "    k_s = 50 # size of kernel for smoothing translations (in number of rows)\n",
    "    n_r = 0.5 # number of rows after which the standard deviation of the translations are 1\n",
    "    # Smoothing kernel for the translations\n",
    "    kernel = signal.gaussian(k_s * shape[1], k_s * shape[1] / 2 ** (5/2))\n",
    "    kernel /= kernel.sum()\n",
    "    \n",
    "    # Create horizontal and vertical translations\n",
    "    trans_row = np.cumsum(np.random.normal(0, 1 / np.sqrt(n_r * shape[1]), size=shape[0] * shape[1]))\n",
    "    trans_col = np.cumsum(np.random.normal(0, 1 / np.sqrt(n_r * shape[1]), size=shape[0] * shape[1]))\n",
    "    trans_row = np.rint(np.convolve(trans_row, kernel, mode=\"same\").reshape(shape))\n",
    "    trans_col = np.rint(np.convolve(trans_col, kernel, mode=\"same\").reshape(shape))\n",
    "\n",
    "    # Warp gaussians and segmentations defining neurons\n",
    "    wrp_gaussian = np.zeros_like(gaussians)\n",
    "    wrp_seg = np.zeros_like(neuron_segs)\n",
    "    for r in range(wrp_gaussian.shape[1]):\n",
    "        for c in range(wrp_gaussian.shape[2]):        \n",
    "            trans_r = int(r + trans_row[r,c])\n",
    "            trans_c = int(c + trans_col[r,c])\n",
    "\n",
    "            # Sample if inside the image, else will be 0s\n",
    "            if 0 < trans_r and trans_r < wrp_gaussian.shape[1] - 1 and \\\n",
    "               0 < trans_c and trans_c < wrp_gaussian.shape[2] - 1:\n",
    "                wrp_gaussian[:, r, c] = gaussians[:, trans_r, trans_c]\n",
    "                wrp_seg[:, r, c] = neuron_segs[:, trans_r, trans_c]\n",
    "    \n",
    "    for i in range(n_neurons):\n",
    "        # Normalize the gaussians\n",
    "        wrp_gaussian[i] /= wrp_gaussian[i].sum()\n",
    "        # If warping the segmentation created multiple ROIs, only keep the largest one\n",
    "        labels, num = measure.label(wrp_seg[i], connectivity=1, return_num=True)\n",
    "        if num > 1:\n",
    "            regions = measure.regionprops(labels)\n",
    "            areas = [region.area for region in regions]\n",
    "            largest_label = regions[np.argmax(areas)].label\n",
    "            wrp_seg[i][labels != largest_label] = 0\n",
    "        # Fill the possible holes in the warped segmentation\n",
    "        wrp_seg[i] = flood_fill(np.pad(wrp_seg[i], 1, 'constant'), fill_val=wrp_seg[i].max())[1:-1, 1:-1]\n",
    "            \n",
    "    return wrp_gaussian, wrp_seg\n",
    "\n",
    "\n",
    "def sample_neurons(i, n_neurons, shape, n_samples, wrp_gaussian, wrp_seg,\n",
    "                   tdTom_max, gcamp_dynamics, laser_gain):\n",
    "    \"\"\"Sample from the gaussians defining neurons for frame `i`.\"\"\"\n",
    "    wrp_neuron = np.zeros(shape + (3,), dtype=wrp_gaussian.dtype)\n",
    "    for j in range(n_neurons):\n",
    "        # Only sample if neuron is in image\n",
    "        if wrp_seg[j].max() == 0:\n",
    "            continue\n",
    "        for c in [0,1]:\n",
    "            if c == 0:\n",
    "                max_neuron = tdTom_max[j]\n",
    "            elif c == 1:\n",
    "                max_neuron = gcamp_dynamics[j,i]\n",
    "            # Only if channel is not 0 (with a tolerance)\n",
    "            if max_neuron < 1e-8: \n",
    "                continue\n",
    "            # Sample from gaussians\n",
    "            # Sampling is adjusted for neuron's intensity, size, and laser gain\n",
    "            if laser_gain < _GAIN_T or c == 0: # low gain or tdTomato, no reduced sampling by laser gain\n",
    "                x = np.random.choice(np.arange(shape[0] * shape[1]), \n",
    "                                     size=int(n_samples[j,c] * \\\n",
    "                                              max_neuron ** 0.5 * (np.count_nonzero(wrp_seg[j]) / 150)), \n",
    "                                     p=wrp_gaussian[j].ravel())\n",
    "            else: # high gain, reduced sampling\n",
    "                x = np.random.choice(np.arange(shape[0] * shape[1]), \n",
    "                                     size=int(n_samples[j,c] * (1 - 0.5 * (laser_gain - _GAIN_T) / (1 - _GAIN_T)) * \\\n",
    "                                              max_neuron ** 0.5 * (np.count_nonzero(wrp_seg[j]) / 150)), \n",
    "                                     p=wrp_gaussian[j].ravel())\n",
    "            # If no sampling (area too small, low intensity, and high gain for e.g.), continue \n",
    "            if x.size == 0:\n",
    "                continue\n",
    "            y, x = np.unravel_index(x, shape)\n",
    "            hist = plt.hist2d(x, y, bins=[shape[1], shape[0]], range=[[0, shape[1]], [0, shape[0]]])[0]\n",
    "            plt.close()\n",
    "\n",
    "            # Adjust maximum and saturation\n",
    "            hist = hist.T / hist.max() * (max_neuron + (1 - max_neuron) * laser_gain / _GAIN_T)\n",
    "            if laser_gain >= _GAIN_T: # high gain, saturation occuring\n",
    "                hist = hist.clip(0, np.percentile(hist[hist > 0], \n",
    "                                                  100 * (1 - (laser_gain - _GAIN_T) / (1 - _GAIN_T)) + \\\n",
    "                                                  80 * (laser_gain - _GAIN_T) / (1 - _GAIN_T)))\n",
    "                hist /= hist.max()\n",
    "            hist = hist.clip(0,1)\n",
    "\n",
    "            wrp_neuron[...,c] = np.maximum(wrp_neuron[...,c], hist)\n",
    "    \n",
    "    return wrp_neuron\n",
    "\n",
    "\n",
    "def reduce_with_border(wrp_seg, return_label):\n",
    "    \"\"\"Reduce the segmentation to one image, while making sure ROIs are separated.\"\"\"\n",
    "    # Compute number of neurons inside the frame\n",
    "    n_neurons = 0\n",
    "    for j in range(wrp_seg.shape[0]):\n",
    "        if np.count_nonzero(wrp_seg[j]) > 0:\n",
    "            n_neurons += 1\n",
    "    # Make sure that touching warped neurons are not segmented together\n",
    "    # and reduce the segmentation to one image per frame\n",
    "    _, num = measure.label(wrp_seg.max(0).astype(np.bool), return_num=True, connectivity=1)\n",
    "    if num < n_neurons:\n",
    "        # If so, introduce a background border with watershed\n",
    "        dist = []\n",
    "        dist_label = []\n",
    "        for j in range(wrp_seg.shape[0]):\n",
    "            if np.count_nonzero(wrp_seg[j]) == 0:\n",
    "                continue\n",
    "            dist.append(ndi.distance_transform_edt(wrp_seg[j]))\n",
    "            dist_label.append(wrp_seg[j].max())\n",
    "        dist = np.array(dist)\n",
    "\n",
    "        local_maxi = np.zeros(shape, dtype=np.uint8)\n",
    "        for j in range(dist.shape[0]):\n",
    "            r,c = np.unravel_index(np.argmax(dist[j], axis=None), dist[j].shape)\n",
    "            local_maxi[r,c] = dist_label[j]\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            labels = morph.watershed(-dist.max(0), local_maxi, mask=wrp_seg.max(0).astype(np.bool), \n",
    "                                     watershed_line=True)\n",
    "        if return_label:\n",
    "            wrp_seg = labels\n",
    "        else:\n",
    "            wrp_seg = labels.astype(np.bool)\n",
    "    else:\n",
    "        wrp_seg = wrp_seg.max(0)\n",
    "    \n",
    "    return wrp_seg\n",
    "\n",
    "\n",
    "def warp_neurons(n_images, n_neurons, shape, gaussians, neuron_segs, return_label,\n",
    "                 tdTom_max, gcamp_dynamics, laser_gain, cyan_gcamp):\n",
    "    \"\"\"Return the warped and sampled neurons and segmentations.\"\"\"\n",
    "    # Number of samples for each neuron (empirically tuned)\n",
    "    n_samples = np.random.normal(loc=1000, scale=200, size=n_neurons * 2).reshape([-1, 2])\n",
    "    n_samples = (n_samples + 0.5).astype(np.uint16)\n",
    "    \n",
    "    wrp_segs = np.zeros((n_images,) + shape, dtype=neuron_segs.dtype)\n",
    "    wrp_neurons = np.zeros((n_images,) + shape + (3,), dtype=gaussians.dtype)\n",
    "    for i in range(n_images):\n",
    "        # Deform gaussians and segmentations\n",
    "        wrp_gaussian, wrp_seg = deform_neurons(n_neurons, shape, gaussians, neuron_segs)\n",
    "        \n",
    "        # Sample neurons\n",
    "        wrp_neurons[i] = sample_neurons(i, n_neurons, shape, n_samples, wrp_gaussian, wrp_seg,\n",
    "                                        tdTom_max, gcamp_dynamics, laser_gain)\n",
    "        \n",
    "        # Reduce segmentations to one image\n",
    "        wrp_segs[i] = reduce_with_border(wrp_seg, return_label)\n",
    "        \n",
    "    # Make GCaMP cyan if applicable\n",
    "    if cyan_gcamp:\n",
    "        wrp_neurons[...,2] = wrp_neurons[...,1]\n",
    "        \n",
    "    return wrp_neurons, wrp_segs\n",
    "\n",
    "\n",
    "def create_noise(n_images, shape, laser_gain, cyan_gcamp):\n",
    "    \"\"\"Create noisy background for all frames.\"\"\"\n",
    "    if laser_gain < _GAIN_T: # low gain, no saturation\n",
    "        noise_means = np.array([_BKG_MEAN_R * (1 - laser_gain) + 0.14 * laser_gain,\n",
    "                                _BKG_MEAN_G * (1 - laser_gain) + 0.13 * laser_gain])\n",
    "        noise_means = np.array([np.random.normal(loc=noise_means[0], scale=_BKG_STD),\n",
    "                                np.random.normal(loc=noise_means[1], scale=_BKG_STD)])\n",
    "\n",
    "        noise_tdTom = np.random.exponential(noise_means[0], size=(n_images,) + shape)\n",
    "        noise_gcamp = (np.random.binomial(1, 1 + (0.03 - 1) * laser_gain ** 0.5, size=(n_images,) + shape) * \\\n",
    "                       np.random.exponential(noise_means[1], size=(n_images,) + shape))\n",
    "\n",
    "    else: # high gain, saturation and reduced sampling\n",
    "        noise_means = np.array([_BKG_MEAN_R * (1 - laser_gain) + 0.14 * laser_gain,\n",
    "                                (_BKG_MEAN_G * (1 - _GAIN_T) + 0.13 * _GAIN_T) * \\\n",
    "                                (1 - (laser_gain - _GAIN_T) ** 2 / (1 - _GAIN_T) ** 2) + \\\n",
    "                                0.4 * (laser_gain - _GAIN_T) ** 2 / (1 - _GAIN_T) ** 2])\n",
    "        noise_means = np.array([np.random.normal(loc=noise_means[0], scale=_BKG_STD),\n",
    "                                np.random.normal(loc=noise_means[1], scale=_BKG_STD)])\n",
    "\n",
    "        noise_tdTom = np.maximum(\n",
    "            np.random.exponential(noise_means[0], size=(n_images,) + shape),\n",
    "            np.random.binomial(1, 0.004 * (laser_gain - _GAIN_T) / (1 - _GAIN_T), size=(n_images,) + shape))\n",
    "        noise_gcamp = np.maximum(\n",
    "            np.random.binomial(1, 1 + (0.03 - 1) * laser_gain ** 0.5, size=(n_images,) + shape) * \\\n",
    "            np.random.exponential(noise_means[1], size=(n_images,) + shape),\n",
    "            np.random.binomial(1, 0.005 * (laser_gain - _GAIN_T) / (1 - _GAIN_T), size=(n_images,) + shape))\n",
    "    \n",
    "    # Make GCaMP cyan if applicable\n",
    "    if cyan_gcamp:\n",
    "        noise = np.stack([noise_tdTom, noise_gcamp, noise_gcamp], -1).clip(0,1)\n",
    "    else:\n",
    "        noise = np.stack([noise_tdTom, noise_gcamp, np.zeros_like(noise_gcamp)], -1).clip(0,1)\n",
    "    \n",
    "    return noise\n",
    "\n",
    "\n",
    "def synthetic_stack(shape, n_images, n_neurons, cyan_gcamp=False, return_label=False):\n",
    "    \"\"\"\n",
    "    Return a stack of synthetic neural images.\n",
    "    \n",
    "    Args:\n",
    "        shape: tuple of int\n",
    "            Tuple (height, width) representing the shape of the images.\n",
    "        n_images: int\n",
    "            Number of images in the stack.\n",
    "        n_neurons: int\n",
    "            Number of neurons to be present on the stack.\n",
    "        cyan_gcamp: bool (default = False)\n",
    "            If True, the GCaMP will appear cyan (same in green and blue channel).\n",
    "            Else, it will be green.\n",
    "        return_label: bool (default = False)\n",
    "            If True, synth_seg will be the labels of the neurons instead of just\n",
    "            their segmentations.\n",
    "            \n",
    "    Returns:\n",
    "        synth_stack: ndarray of shape NxHxWx3\n",
    "            Stack of N synthetic images.\n",
    "        synth_seg: ndarray of shape NxHxW\n",
    "            Stack of N synthetic segmentations (or label, see `return_label`).\n",
    "    \"\"\" \n",
    "    ## Initialization\n",
    "    # GCaMP type (50% 6f and 50% 6s)\n",
    "    if np.random.rand() < 0.5:\n",
    "        gcamp_type = \"6f\"\n",
    "    else: # GCaMP6s\n",
    "        gcamp_type = \"6s\"\n",
    "    # Laser gain, from 0 to 1 <=> low to high\n",
    "    laser_gain = 1 - 0.5 * signal.gaussian(100, 20)\n",
    "    laser_gain = np.random.choice(np.arange(100) / 100, p=laser_gain / laser_gain.sum())\n",
    "    \n",
    "    # Create the gaussians representing the neurons\n",
    "    gaussians, neuron_segs = create_neurons(n_neurons, shape, return_label)\n",
    "    \n",
    "    # Choose which channels are present in each neurons\n",
    "    tdTom_max, gcamp_dynamics = get_flurophores(n_neurons, n_images, gcamp_type)\n",
    "    \n",
    "    # Warp neurons for each image to create the stack\n",
    "    wrp_neurons, wrp_segs = warp_neurons(n_images, n_neurons, shape, gaussians, neuron_segs, return_label,\n",
    "                                         tdTom_max, gcamp_dynamics, laser_gain, cyan_gcamp)\n",
    "                    \n",
    "    # Add background noise\n",
    "    noise = create_noise(n_images, shape, laser_gain, cyan_gcamp)\n",
    "    \n",
    "    # Put neurons and noise together\n",
    "    synth_stack = np.maximum(wrp_neurons, noise)\n",
    "    for c in [0,1,2]:\n",
    "        if c == 2 and cyan_gcamp is False:\n",
    "            continue\n",
    "        synth_stack[...,c] /= synth_stack[...,c].max()\n",
    "    synth_seg = wrp_segs\n",
    "    \n",
    "    # Random gamma correction (image should be in [0,1] range)\n",
    "    gamma = np.random.rand() * 0.6 + 0.7 # in [0.7, 1.3)\n",
    "    synth_stack = exposure.adjust_gamma(synth_stack, gamma=gamma)\n",
    "    \n",
    "    return synth_stack, synth_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack generated in 7 seconds (0.301 s/image).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b156c2f63474f02a2d04566721e4e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=24), Output()), _dom_classes=('widget-interaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_images = 25\n",
    "\n",
    "n_neurons = np.random.randint(2, 6 + 1)\n",
    "if np.random.rand() < 0.5: # square image half of the time\n",
    "    rand_size = np.random.randint(6, 10 + 1) * 32\n",
    "    shape = (rand_size, rand_size)\n",
    "else:\n",
    "    rand_h = np.random.randint(6, 10 + 1) * 32\n",
    "    rand_w = np.random.randint(rand_h/32, 10 + 1) * 32\n",
    "    shape = (rand_h, rand_w)\n",
    "\n",
    "start = time.time()\n",
    "synth_stack, synth_seg = synthetic_stack(shape, n_images, n_neurons, cyan_gcamp=True, return_label=True)\n",
    "end = time.time()\n",
    "print(\"Stack generated in %d seconds (%.3f s/image).\" % (end - start, (end-start) / n_images))\n",
    "\n",
    "vmax = synth_seg.max()\n",
    "@interact(image = (0, n_images - 1))\n",
    "def plot_data(image=0):\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(synth_stack[image], vmin=0, vmax=1, cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(synth_seg[image], cmap='viridis', vmax=vmax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save synthetic stacks\n",
    "Save some synthetic stacks in the dataset. Keep them in a separate folder \"synthetic/\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_neurons = 2\n",
    "n_stacks = 78\n",
    "n_images = 600\n",
    "synth_dir = \"/data/talabot/pdm/dataset/synthetic/\"\n",
    "date = time.strftime(\"%y%m%d\", time.localtime())\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(n_stacks):\n",
    "    folder = os.path.join(synth_dir, \"synth_{}neur_{:03d}\".format(n_neurons, i))\n",
    "    print(\"Creating stack %d/%d\" % (i + 1, n_stacks), end=\"\")\n",
    "    print(\"  - folder:\", folder)\n",
    "\n",
    "    synth_stack, synth_seg = synthetic_stack(shape, n_images, n_neurons=n_neurons)\n",
    "\n",
    "    # Change synth_stack to red in RGB mode (to be consistent with deep learning code)\n",
    "    synth_stack = gray2red(synth_stack)\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    os.makedirs(os.path.join(folder, \"rgb_frames\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(folder, \"seg_frames\"), exist_ok=True)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Save full stacks\n",
    "        io.imsave(os.path.join(folder, \"RGB.tif\"), to_npint(synth_stack))\n",
    "        io.imsave(os.path.join(folder, \"seg_ROI.tif\"), to_npint(synth_seg))\n",
    "        # Save image per image\n",
    "        for j in range(n_images):\n",
    "            io.imsave(os.path.join(folder, \"rgb_frames\", \"rgb_{:04}.png\".format(j)), to_npint(synth_stack[j]))\n",
    "            io.imsave(os.path.join(folder, \"seg_frames\", \"seg_{:04}.png\".format(j)), to_npint(synth_seg[j]))\n",
    "\n",
    "duration = time.time() - start\n",
    "print(\"\\nScript took {:02.0f}min {:02.0f}s.\".format(duration // 60, duration % 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
