{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic generation\n",
    "Notebook used to make whole stacks of synthetic data based on single step results (synthetic_tests.ipynb).  \n",
    "\n",
    "**Note:** the script `run_generation.py` is used to create the stacks. Functions are then written both here and in the script. The script should be the latest working version, whereas here should be for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, time, pickle\n",
    "import warnings\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, draw, color, exposure\n",
    "import skimage.morphology as morph\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import signal\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from utils_common.image import to_npint, gray2red\n",
    "from utils_common.processing import flood_fill\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following are pre-computed on real data. See stats_###.pkl & README.md.\n",
    "_BKG_MEAN_R = 0.04061809239988313 # mean value of red background (190222)\n",
    "_BKG_MEAN_G = 0.03090710807146899 # mean value of red background (190222)\n",
    "_BKG_STD = 0.005 # Standard deviation of mean value of background (empirically tuned)\n",
    "_ROI_MAX_1 = 0.2276730082246407 # fraction of red ROI with 1 as max intensity (181121)\n",
    "_ROI_MAX_MEAN = 0.6625502112855037 # mean of red ROI max (excluding 1.0) (181121)\n",
    "_ROI_MAX_STD = 0.13925117610178622 # std of red ROI max (excluding 1.0) (181121)\n",
    "\n",
    "# Following are the pre-generated GCaMP response kernel\n",
    "with open(\"GCaMP_kernel.pkl\", \"rb\") as f:\n",
    "    kernel_f, kernel_s = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one\n",
    "Function to create a synthetic image/stack from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_stack(shape, n_images, n_neurons):\n",
    "    \"\"\"\n",
    "    Return a stack of synthetic neural images.\n",
    "    \n",
    "    Args:\n",
    "        shape: tuple of int\n",
    "            Tuple (height, width) representing the shape of the images.\n",
    "        n_images: int\n",
    "            Number of images in the stack.\n",
    "        n_neurons: int\n",
    "            Number of neurons to be present on the stack.\n",
    "            \n",
    "    Returns:\n",
    "        synth_stack: ndarray of shape NxHxWx3\n",
    "            Stack of N synthetic images.\n",
    "        synth_seg: ndarray of shape NxHxW\n",
    "            Stack of N synthetic segmentations.\n",
    "    \"\"\" \n",
    "    # Initialization\n",
    "    ellipse_size = 1.5 # factor for the ground truth ellipse (normalized by std)\n",
    "    # Number of samples for each neuron (empirically tuned)\n",
    "    n_samples = np.random.normal(loc=1000, scale=200, size=n_neurons * 2).reshape([-1, 2])\n",
    "    n_samples = (n_samples + 0.5).astype(np.uint16)\n",
    "    fps = 2.4 # synthetic frame per seconds\n",
    "    # For warping (like acquisition process):\n",
    "    k_s = 50 # size of kernel for smoothing translations (in number of rows)\n",
    "    n_r = 0.5 # number of rows after which the standard deviation of the translations are 1\n",
    "    \n",
    "    ## Create the gaussians representing the neurons\n",
    "    gaussians = np.zeros((n_neurons,) + shape)\n",
    "    neurons_seg = np.zeros(shape, dtype=np.bool)\n",
    "    # Meshgrid for the gaussian weights\n",
    "    rows, cols = np.arange(shape[0]), np.arange(shape[1])\n",
    "    meshgrid = np.zeros(shape + (2,))\n",
    "    meshgrid[:,:,0], meshgrid[:,:,1] = np.meshgrid(cols, rows) # note the order\n",
    "    for i in range(n_neurons):\n",
    "        # Loop until the randomly generated neuron is in the image \n",
    "        # and doesn't overlap with another (can theoretically loop to infinity)\n",
    "        while True:\n",
    "\n",
    "            # Mean and covariance matrix of gaussian (empirically tuned)\n",
    "            # Note that x and y axes are col and row (so, inversed!)\n",
    "            mean = np.array([np.random.randint(shape[1]), np.random.randint(shape[0])])\n",
    "            scale_x = shape[1] / 64\n",
    "            scale_y = shape[0] / 64\n",
    "            cross_corr = np.random.randint(-2, 3) * min(scale_x, scale_y)\n",
    "            cov = np.array([[np.random.randint(1, 4) * scale_x, cross_corr],\n",
    "                            [cross_corr, np.random.randint(10, 40) * scale_y]])\n",
    "\n",
    "            # Bounding ellipse\n",
    "            val, vec = np.linalg.eig(cov)\n",
    "            rotation = math.atan2(vec[0, np.argmax(val)], vec[1, np.argmax(val)])\n",
    "            rr, cc = draw.ellipse(mean[1], mean[0], \n",
    "                                  ellipse_size * np.sqrt(val[1]), \n",
    "                                  ellipse_size * np.sqrt(val[0]),\n",
    "                                  rotation=rotation)\n",
    "            # Check if outside the image\n",
    "            if (rr < 0).any() or (rr >= shape[0]).any() or (cc < 0).any() or (cc >= shape[1]).any():\n",
    "                continue\n",
    "            else:\n",
    "                # Check if overlapping/touching with any existing neuron\n",
    "                tmp_mask = np.zeros(shape, dtype=np.bool)\n",
    "                tmp_mask[rr,cc] = 1\n",
    "                if (neurons_seg[morph.dilation(tmp_mask)] == True).any():\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "        neurons_seg[rr, cc] = True\n",
    "        \n",
    "        # Create gaussian weight image\n",
    "        gaussians[i,:,:] = multivariate_normal.pdf(meshgrid, mean, cov)\n",
    "        gaussians[i,:,:] /= gaussians[i,:,:].sum()\n",
    "\n",
    "    # Choose which channels are present in each neurons\n",
    "    c_presence = np.array([[True, True], [True, False], [False, True]], dtype=np.bool)\n",
    "    channel_neurons = c_presence[np.random.choice(len(c_presence), size=n_neurons, p=[0.9, 0.05, 0.05]), :]\n",
    "    \n",
    "    # RED: choose max intensity of the neurons\n",
    "    red_max = np.zeros(n_neurons)  \n",
    "    for i in range(n_neurons):\n",
    "        if channel_neurons[i, 0] == False:\n",
    "            continue\n",
    "        # Sample randomly the neuron maximum \n",
    "        if np.random.rand() < _ROI_MAX_1:\n",
    "            red_max[i] = 1.0\n",
    "        else:\n",
    "            loc = _ROI_MAX_MEAN\n",
    "            scale = _ROI_MAX_STD\n",
    "            red_max[i] = np.clip(np.random.normal(loc=loc, scale=scale), 0, 1)\n",
    "    \n",
    "    # GREEN: create dynamics through time of the neurons\n",
    "    green_dynamics = np.zeros((n_neurons, n_images))\n",
    "    # GCaMP type\n",
    "    if np.random.rand() < 0.5: # GCaMP6f\n",
    "        kernel_gcamp = kernel_f\n",
    "    else: # GCaMP6s\n",
    "        kernel_gcamp = kernel_s\n",
    "    t = np.arange(np.ceil(n_images / fps) * 1000) / 1000 # in ms\n",
    "    for i in range(n_neurons):\n",
    "        if channel_neurons[i, 1] == False:\n",
    "            continue\n",
    "        # Rate of firing\n",
    "        rate = np.zeros(len(t))\n",
    "        rate[np.random.randint(len(t), size=n_images // 10)] = 0.5\n",
    "        rate = np.convolve(rate, signal.gaussian(5000, 1000), mode='full')[:len(rate)].clip(0,1)\n",
    "        # Spiking or non-spiking\n",
    "        if np.random.rand() < 0.8:\n",
    "            spikes = np.random.poisson(rate / 250, size=len(t)).clip(0,1)\n",
    "            dynamics = np.convolve(spikes, kernel_gcamp, mode=\"full\")[:len(spikes)]\n",
    "        else:\n",
    "            dynamics = np.convolve(rate / 100, kernel_gcamp, mode=\"full\")[:len(rate)]\n",
    "        # Sub-sample to fps\n",
    "        green_dynamics[i] = dynamics[::int(np.rint(1000/fps))][:n_images]\n",
    "        # If no red, assures a minimum to avoid invisible neurons\n",
    "        if channel_neurons[i,0]:\n",
    "            green_dynamics[i] = green_dynamics[i].clip(0,1)\n",
    "        else:\n",
    "            green_dynamics[i] = green_dynamics[i].clip(np.random.normal(0.4, 0.02),1)\n",
    "    \n",
    "    ## Warp neurons for each image to create the stack\n",
    "    # Smoothing kernel for the translations\n",
    "    kernel = signal.gaussian(k_s * shape[1], k_s * shape[1] / 2 ** (5/2))\n",
    "    kernel /= kernel.sum()\n",
    "    wrp_segs = np.zeros((n_images,) + shape, dtype=np.bool)\n",
    "    wrp_neurons = np.zeros((n_images,) + shape + (3,), dtype=gaussians.dtype)\n",
    "    for i in range(n_images):\n",
    "        # Create horizontal and vertical translations\n",
    "        trans_row = np.cumsum(np.random.normal(0, 1 / np.sqrt(n_r * shape[1]), size=shape[0] * shape[1]))\n",
    "        trans_col = np.cumsum(np.random.normal(0, 1 / np.sqrt(n_r * shape[1]), size=shape[0] * shape[1]))\n",
    "        trans_row = np.rint(np.convolve(trans_row, kernel, mode=\"same\").reshape(shape))\n",
    "        trans_col = np.rint(np.convolve(trans_col, kernel, mode=\"same\").reshape(shape))\n",
    "        \n",
    "        # Warp gaussians and segmentation defining neurons\n",
    "        wrp_gaussian = np.zeros_like(gaussians)\n",
    "        for r in range(wrp_gaussian.shape[1]):\n",
    "            for c in range(wrp_gaussian.shape[2]):        \n",
    "                trans_r = int(r + trans_row[r,c])\n",
    "                trans_c = int(c + trans_col[r,c])\n",
    "\n",
    "                # sample if inside the image (else, do nothing, i.e. sample zeros)\n",
    "                if 0 < trans_r and trans_r < wrp_gaussian.shape[1] - 1 and \\\n",
    "                   0 < trans_c and trans_c < wrp_gaussian.shape[2] - 1:\n",
    "                    wrp_gaussian[:, r, c] = gaussians[:, trans_r, trans_c]\n",
    "                    wrp_segs[i, r, c] = neurons_seg[trans_r, trans_c]\n",
    "        \n",
    "        # Fill the possible holes in the warped segmentation\n",
    "        wrp_segs[i] = flood_fill(np.pad(wrp_segs[i], 1, 'constant'))[1:-1, 1:-1]\n",
    "        \n",
    "        # Sample neurons\n",
    "        for j in range(n_neurons):\n",
    "            wrp_gaussian[j] /= wrp_gaussian[j].sum()\n",
    "            \n",
    "            for c in [0,1]:\n",
    "                if channel_neurons[j,c] == False:\n",
    "                    continue\n",
    "                # Sample from gaussians\n",
    "                x = np.random.choice(shape[0] * shape[1], size=n_samples[j,c], p=wrp_gaussian[j].ravel())\n",
    "                y, x = np.unravel_index(x, shape)\n",
    "                hist = plt.hist2d(x, y, bins=[shape[1], shape[0]], range=[[0, shape[1]], [0, shape[0]]])[0]\n",
    "                plt.close()\n",
    "                if c == 0:\n",
    "                    max_neuron = red_max[j]\n",
    "                elif c == 1:\n",
    "                    max_neuron = green_dynamics[j,i]\n",
    "                wrp_neurons[i,...,c] = np.maximum(wrp_neurons[i,...,c], hist.T / hist.max() * max_neuron)\n",
    "    \n",
    "    ## Add noise (sampled from an exponential distribution)\n",
    "    noise_means = np.array([np.random.normal(loc=_BKG_MEAN_R, scale=_BKG_STD),\n",
    "                            np.random.normal(loc=_BKG_MEAN_G, scale=_BKG_STD)])\n",
    "    noise = np.stack([np.random.exponential(scale=noise_means[0], size=(n_images,) + shape),\n",
    "                      np.random.exponential(scale=noise_means[1], size=(n_images,) + shape),\n",
    "                      np.zeros((n_images,) + shape)], -1)\n",
    "    \n",
    "    synth_stack = np.maximum(wrp_neurons, noise)\n",
    "    for c in [0,1]:\n",
    "        synth_stack[...,c] /= synth_stack[...,c].max()\n",
    "    synth_seg = wrp_segs\n",
    "    \n",
    "    ## Random gamma correction (image should be in [0,1] range)\n",
    "    gamma = np.random.rand() * 0.6 + 0.7 # in [0.7, 1.3)\n",
    "    synth_stack = exposure.adjust_gamma(synth_stack, gamma=gamma)\n",
    "    \n",
    "    return synth_stack, synth_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack generated in 5 seconds (0.237 s/image).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce59b02b06941feb8404c58f36eb00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=24), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_images = 25\n",
    "\n",
    "n_neurons = np.random.randint(2, 6 + 1)\n",
    "if np.random.rand() < 0.5: # square image half of the time\n",
    "    rand_size = np.random.randint(6, 10 + 1) * 32\n",
    "    shape = (rand_size, rand_size)\n",
    "else:\n",
    "    rand_h = np.random.randint(6, 10 + 1) * 32\n",
    "    rand_w = np.random.randint(rand_h/32, 10 + 1) * 32\n",
    "    shape = (rand_h, rand_w)\n",
    "\n",
    "start = time.time()\n",
    "synth_stack, synth_seg = synthetic_stack(shape, n_images, n_neurons=n_neurons)\n",
    "end = time.time()\n",
    "print(\"Stack generated in %d seconds (%.3f s/image).\" % (end - start, (end-start) / n_images))\n",
    "\n",
    "@interact(image = (0, n_images - 1))\n",
    "def plot_data(image=0):\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(synth_stack[image], vmin=0, vmax=1, cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(synth_seg[image], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save synthetic stacks\n",
    "Save some synthetic stacks in the dataset. Keep them in a separate folder \"synthetic/\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_neurons = 2\n",
    "n_stacks = 78\n",
    "n_images = 600\n",
    "synth_dir = \"/data/talabot/pdm/dataset/synthetic/\"\n",
    "date = time.strftime(\"%y%m%d\", time.localtime())\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(n_stacks):\n",
    "    folder = os.path.join(synth_dir, \"synth_{}neur_{:03d}\".format(n_neurons, i))\n",
    "    print(\"Creating stack %d/%d\" % (i + 1, n_stacks), end=\"\")\n",
    "    print(\"  - folder:\", folder)\n",
    "\n",
    "    synth_stack, synth_seg = synthetic_stack(shape, n_images, n_neurons=n_neurons)\n",
    "\n",
    "    # Change synth_stack to red in RGB mode (to be consistent with deep learning code)\n",
    "    synth_stack = gray2red(synth_stack)\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    os.makedirs(os.path.join(folder, \"rgb_frames\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(folder, \"seg_frames\"), exist_ok=True)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Save full stacks\n",
    "        io.imsave(os.path.join(folder, \"RGB.tif\"), to_npint(synth_stack))\n",
    "        io.imsave(os.path.join(folder, \"seg_ROI.tif\"), to_npint(synth_seg))\n",
    "        # Save image per image\n",
    "        for j in range(n_images):\n",
    "            io.imsave(os.path.join(folder, \"rgb_frames\", \"rgb_{:04}.png\".format(j)), to_npint(synth_stack[j]))\n",
    "            io.imsave(os.path.join(folder, \"seg_frames\", \"seg_{:04}.png\".format(j)), to_npint(synth_seg[j]))\n",
    "\n",
    "duration = time.time() - start\n",
    "print(\"\\nScript took {:02.0f}min {:02.0f}s.\".format(duration // 60, duration % 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
