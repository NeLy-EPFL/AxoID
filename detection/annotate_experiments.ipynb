{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate experiments\n",
    "Notebook used to create binary annotations for experiments using a computer vision approach, or deep networks.  \n",
    "Manual tuning and correction are used to improve the quality of the detection, before it is used as ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time, shutil\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, filters\n",
    "from skimage.morphology import disk\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils_common.image import imread_to_float, to_npint, overlay_contours\n",
    "from utils_common.processing import nlm_denoising\n",
    "from utils_common.register_cc import register_stack\n",
    "from computer_vision.cv_detector import cv_detect\n",
    "from deep_learning.utils_data import normalize_range, pad_transform_stack\n",
    "from deep_learning.utils_test import predict_stack\n",
    "# from deep_learning\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449be3c4cb654709956409b85b415d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=599), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "animal_to_annotate = os.listdir(\"/data/talabot/experiments/to_annotate/\")\n",
    "animal_annotated = os.listdir(\"/data/talabot/experiments/annotated/\")\n",
    "\n",
    "# Randomly select an animal without any annotated experiment\n",
    "animal = np.random.choice([animal for animal in animal_to_annotate if animal not in animal_annotated])\n",
    "# Randomly select an experiment to annotated\n",
    "experiment = np.random.choice(os.listdir(os.path.join(\"/data/talabot/experiments/to_annotate/\", animal)))\n",
    "\n",
    "# Load RGB stack and register it\n",
    "rgb_stack = imread_to_float(os.path.join(\"/data/talabot/experiments/to_annotate/\", animal, experiment, \"RGB.tif\"))\n",
    "reg_rgb = register_stack(rgb_stack, ref_num=0)\n",
    "\n",
    "@interact(image=(0, len(rgb_stack) - 1))\n",
    "def plot_stack(image=0):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.suptitle(\"Animal: %s\\nExp: %s\" % (animal, experiment))\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"Raw frame %d\" % image)\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Mean temporal registered image\")\n",
    "    plt.imshow(reg_rgb.mean(0))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoising\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eee37b69614f16998a0b4599e9d72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=599), BoundedIntText(value=11, description='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97e1065bd3e46449734a31c68c9e166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='image', max=597, min=2), BoundedIntText(value=11, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the denoising and detector to tune their parameters\n",
    "print(\"Denoising\")\n",
    "@interact(image=(0, len(rgb_stack) - 1),\n",
    "          h_red=widgets.BoundedIntText(value=11, min=0),\n",
    "          h_green=widgets.BoundedIntText(value=11, min=0))\n",
    "def plot_denoising(image=0, h_red=11, h_green=11):\n",
    "    start = time.time()\n",
    "    denoised = nlm_denoising(rgb_stack, img_id=image, h_red=h_red, h_green=h_green)\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Denoised\\n(Took %f s.)\" % duration)\n",
    "    plt.imshow(denoised)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Detection\")\n",
    "@interact(image=(2, len(rgb_stack) - 3),\n",
    "          h_red=widgets.BoundedIntText(value=11, min=0),\n",
    "          h_green=widgets.BoundedIntText(value=11, min=0),\n",
    "          thresh_fn=widgets.ToggleButtons(options=[\"Otsu\", \"Li\", \"Constant\"]),\n",
    "          thresh_val=widgets.BoundedIntText(value=30, min=0, max=255),\n",
    "          erosion=(0,5))\n",
    "def plot_detector(image=0, h_red=11, h_green=11, thresh_fn=\"Otsu\", thresh_val=30, erosion=0):\n",
    "    if thresh_fn == \"Otsu\":\n",
    "        thresholding_fn = filters.threshold_otsu\n",
    "    elif thresh_fn == \"Li\":\n",
    "        thresholding_fn = filters.threshold_li\n",
    "    elif thresh_fn == \"Constant\":\n",
    "        thresholding_fn = lambda x: thresh_val / 255\n",
    "    \n",
    "    start = time.time()\n",
    "    detection = cv_detect(rgb_stack[image - 2: image + 3], \n",
    "                          h_red=h_red, h_green=h_green,\n",
    "                          thresholding_fn=thresholding_fn, \n",
    "                          registration=False, selem=disk(erosion))[2]\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    plt.figure(figsize=(13,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Detection\\n(Took %f s.)\" % duration)\n",
    "    plt.imshow(detection)\n",
    "    plt.subplot(133)\n",
    "    plt.title(\"Detection contours\")\n",
    "    plt.imshow(overlay_contours(rgb_stack[image], detection))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV detection took 3 min 33 s.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dae04e88f848ffa4eb2edc4b8d770b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=599), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply detector to the whole stack (/!\\ a bit slow)\n",
    "h_red = 12\n",
    "h_green = 12\n",
    "thresholding_fn = filters.threshold_otsu\n",
    "erosion = 1\n",
    "\n",
    "start = time.time()\n",
    "cv_detection = cv_detect(rgb_stack, h_red=h_red, h_green=h_green, thresholding_fn=thresholding_fn, \n",
    "                         registration=False, selem=disk(erosion))\n",
    "duration = time.time() - start\n",
    "duration_str = \"%d min %d s\" % (duration // 60, duration % 60)\n",
    "print(\"CV detection took %s.\" % duration_str)\n",
    "\n",
    "@interact(image=(0, len(cv_detection) - 1))\n",
    "def plot_detector(image=0):\n",
    "    plt.figure(figsize=(13,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Detection\")\n",
    "    plt.imshow(cv_detection[image])\n",
    "    plt.subplot(133)\n",
    "    plt.title(\"Detection contours\")\n",
    "    plt.imshow(overlay_contours(rgb_stack[image], cv_detection[image]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Prediction took 1.1 s.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e42e0648334731a801fdb575160c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=599), FloatSlider(value=0.5, description='de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"RG_synth_190311\"\n",
    "\n",
    "model_dir = \"deep_learning/models/\"\n",
    "input_channels = \"RG\"\n",
    "u_depth = 4\n",
    "out1_channels = 16\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load model\n",
    "sys.path.append(os.path.join(model_dir, model_name))\n",
    "from utils_model_save import CustomUNet as ModelNet\n",
    "\n",
    "model = ModelNet(len(input_channels), u_depth=u_depth, out1_channels=out1_channels, batchnorm=True, device=device)\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, model_name, \"model_best.pth\")))\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "transform = lambda stack: normalize_range(pad_transform_stack(stack, u_depth))\n",
    "start = time.time()\n",
    "predictions = predict_stack(model, rgb_stack, batch_size, input_channels=input_channels, \n",
    "                            channels_last=True, transform=transform)\n",
    "print(\"Prediction took %.1f s.\" % (time.time() - start))\n",
    "predictions = torch.sigmoid(predictions).numpy()\n",
    "\n",
    "# Display results\n",
    "@interact(image=(0, len(cv_detection) - 1),\n",
    "          detection_th=widgets.FloatSlider(value=0.5, min=0, max=1, step=0.1, readout_format=\".1f\"))\n",
    "def plot_detector(image=0, detection_th=0.5):\n",
    "    detection = predictions[image] > detection_th\n",
    "    \n",
    "    plt.figure(figsize=(13,8))\n",
    "    plt.subplot(231)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    plt.subplot(232)\n",
    "    plt.title(\"Detection\")\n",
    "    plt.imshow(detection)\n",
    "    plt.subplot(233)\n",
    "    plt.title(\"Detection contours\")\n",
    "    plt.imshow(overlay_contours(rgb_stack[image], detection))\n",
    "    plt.subplot(235)\n",
    "    plt.title(\"Prediction probabilities\")\n",
    "    plt.imshow(predictions[image])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f31744618a54481834e22cbe2375421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=599), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make final predictions\n",
    "dl_detection_th = 0.5\n",
    "\n",
    "dl_detection = predictions > dl_detection_th\n",
    "@interact(image=(0, len(cv_detection) - 1))\n",
    "def plot_detector(image=0):    \n",
    "    plt.figure(figsize=(13,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Detection\")\n",
    "    plt.imshow(dl_detection[image])\n",
    "    plt.subplot(133)\n",
    "    plt.title(\"Detection contours\")\n",
    "    plt.imshow(overlay_contours(rgb_stack[image], dl_detection[image]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select and tune best results\n",
    "Compare results of detector, manually tune/correct them, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final detection\n",
    "Save the final detection as ground truths segmentation and move the corresponding experience folder to the annotated directory."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_detection = np.zeros(rgb_stack.shape, dtype=np.bool)\n",
    "\n",
    "# Save as segmentation images\n",
    "folder = os.path.join(\"/data/talabot/experiments/to_annotate/\", animal, experiment)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Save full stacks\n",
    "    io.imsave(os.path.join(folder, \"seg_ROI.tif\"), to_npint(final_detection))\n",
    "    # Save image per image\n",
    "    for i in range(len(final_detection)):\n",
    "        io.imsave(os.path.join(folder, \"seg_frames\", \"seg_{:04}.png\".format(i)), to_npint(final_detection[i]))\n",
    "\n",
    "# Move the experiment to \"annotated/\" directory\n",
    "os.makedirs(os.path.join(\"/data/talabot/experiments/to_annotate/\", animal), exist_ok=True)\n",
    "shutil.move(os.path.join(\"/data/talabot/experiments/to_annotate/\", animal, experiment),\n",
    "            os.path.join(\"/data/talabot/experiments/annotated/\", animal, experiment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
