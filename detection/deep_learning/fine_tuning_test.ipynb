{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning test\n",
    "Test the possibility to manually annotated a few frames of the experiment, then fine tune the network on them to predict the rest of the frames.  \n",
    "This is kind of overfitting part of the test set to perform well on the rest.\n",
    "\n",
    "**Fine tuning:**\n",
    "  * Number of annotated frames --> 1-5 seems about alright but higher is better\n",
    "  * Number in batch --> half of batch is alright (dynamic batch_size with maximum)\n",
    "  * Need previous dataset or not --> seems yes\n",
    "  * Learning rate --> keep same\n",
    "  * Number of iteration --> about 200\n",
    "  * Data aug, synth, weights, ... --> nothing (weights because of high noise?)\n",
    "  \n",
    "**Last layer tuning:** (only fine tune the last convolution)  \n",
    "  --> Does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time, shutil, copy\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils_common.image import imread_to_float, to_npint\n",
    "from utils_common.metrics import dice_coef\n",
    "from utils_data import normalize_range, get_all_dataloaders, pad_transform, pad_transform_stack\n",
    "from utils_loss import get_BCEWithLogits_loss\n",
    "from utils_metric import get_dice_metric\n",
    "from utils_model import CustomUNet\n",
    "from utils_test import predict, predict_stack, evaluate, evaluate_stack\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed*10 + 1234)\n",
    "torch.manual_seed(seed*100 + 4321)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Choose wether or not use synth, aug, and weights for fine tuning\n",
    "synth_data = False\n",
    "synth_ratio = None # ratio of synthetic data vs. real data\n",
    "only_synth = False # If True, will use only the synthetic data (and all of it, at the opposite of ratio=1)\n",
    "data_aug = False # If True, will use data augmentation (see below for augmentation sequence)\n",
    "use_weights = False # if False use class weights, if True use pixelwise weights (if existing)\n",
    "\n",
    "input_channels = \"RG\" # Channel to use as input\n",
    "u_depth = 4\n",
    "out1_channels = 16\n",
    "\n",
    "out_model_name = \"models/test_ft\"\n",
    "model_name = \"models/unet4-16_RG_cv-annotated/\"\n",
    "data_dir = \"/data/talabot/pdm/dataset_cv-annotated/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training\n",
    "Make dataloaders and so on to prepare fine tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random augment sequence for data augmentation if applicable\n",
    "if data_aug:\n",
    "    seq = iaa.GammaContrast((0.7, 1.3)) # Gamma correction\n",
    "    aug_fn = seq.augment_image\n",
    "else:\n",
    "    aug_fn = lambda x: x # identity function\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = get_all_dataloaders(\n",
    "    data_dir,\n",
    "    batch_size, \n",
    "    input_channels = input_channels,\n",
    "    test_dataloader = True,\n",
    "    use_weights = use_weights,\n",
    "    synthetic_data = synth_data, synthetic_ratio = synth_ratio, synthetic_only = only_synth,\n",
    "    train_transform = lambda img: normalize_range(pad_transform(aug_fn(img), u_depth)),\n",
    "    train_target_transform = lambda img: pad_transform(img, u_depth),\n",
    "    eval_transform = lambda img: normalize_range(pad_transform(img, u_depth)), \n",
    "    eval_target_transform = lambda img: pad_transform(img, u_depth)\n",
    ")\n",
    "# \"Deactivate\" the collate_fn of the train dataloader\n",
    "collate_fn = dataloaders[\"train\"].collate_fn\n",
    "dataloaders[\"train\"].collate_fn = lambda batch: batch\n",
    "\n",
    "# Compute class weights (as pixel imbalance)\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "for filename in dataloaders[\"train\"].dataset.y_filenames:\n",
    "    y = io.imread(filename)\n",
    "    pos_count += (y == 255).sum()\n",
    "    neg_count += (y == 0).sum()\n",
    "pos_weight = torch.tensor((neg_count + pos_count) / (2 * pos_count)).to(device)\n",
    "neg_weight = torch.tensor((neg_count + pos_count) / (2 * neg_count)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomUNet(len(input_channels), u_depth=u_depth, \n",
    "                   out1_channels=out1_channels, batchnorm=True, device=device)\n",
    "model.load_state_dict(torch.load(os.path.join(model_name, \"model_best.pth\")))\n",
    "\n",
    "loss_fn = get_BCEWithLogits_loss(pos_weight=pos_weight, neg_weight=neg_weight)\n",
    "metrics = {\"dice\": get_dice_metric()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load experiments and annotated frames\n",
    "Load an experiment, predict once the detections, and create annotations for a few frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted experiment in 3.4 s.\n",
      "Dice = 0.19969367975092164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650533ffe44a4a48b4e7000ee04700a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=567), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = \"/data/talabot/experiments/annotated/R70H06_20181202-tdTomGC6fopt-fl2/R70H06-tdTomGC6fopt-fly2-001/\"\n",
    "\n",
    "# Load experiment and segmentation & weights if available\n",
    "rgb_stack = imread_to_float(os.path.join(experiment, \"RGB.tif\"))\n",
    "if os.path.isfile(os.path.join(experiment, \"seg_ROI.tif\")):\n",
    "    seg_stack = imread_to_float(os.path.join(experiment, \"seg_ROI.tif\"))\n",
    "else:\n",
    "    seg_stack = None\n",
    "if use_weights and os.path.isfile(os.path.join(experiment, \"weights.tif\")):\n",
    "    weights_stack = imread_to_float(os.path.join(experiment, \"weights.tif\"))\n",
    "else:\n",
    "    weights_stack = None\n",
    "\n",
    "# Predict using loaded model\n",
    "start = time.time()\n",
    "predictions = predict_stack(model, rgb_stack, batch_size, input_channels=input_channels,\n",
    "                            transform=lambda stack: normalize_range(pad_transform_stack(stack, u_depth)))\n",
    "predictions = torch.sigmoid(predictions)\n",
    "print(\"Predicted experiment in %.1f s.\" % (time.time() - start))\n",
    "\n",
    "if seg_stack is not None:\n",
    "    print(\"Dice =\", dice_coef((predictions > 0.5).numpy(), seg_stack))\n",
    "\n",
    "@interact(image=(0, len(rgb_stack) - 1))\n",
    "def plot_experiment(image=0):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    if seg_stack is not None:     \n",
    "        plt.subplot(232)\n",
    "        plt.title(\"Binary detection\")\n",
    "        plt.imshow(seg_stack[image], cmap=\"gray\")\n",
    "    if weights_stack is not None:        \n",
    "        plt.subplot(233)\n",
    "        plt.title(\"Pixel weighting\")\n",
    "        plt.imshow(weights_stack[image], cmap=\"gray\")\n",
    "    plt.subplot(235)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(predictions[image], cmap=\"gray\")\n",
    "    plt.subplot(236)\n",
    "    plt.title(\"Binary prediction\")\n",
    "    plt.imshow((predictions[image] > 0.5), cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ground truth as annotations to test how many frames are needed, and how to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of annotated frames:\n",
      "[ 33 165 298]\n"
     ]
    }
   ],
   "source": [
    "# Select and annotated frames\n",
    "n_annotated = 3 # number of frames to annotated\n",
    "\n",
    "# Randomly choose frames\n",
    "indices_annotated = np.random.choice(np.arange(len(rgb_stack)), size=n_annotated, replace=False)\n",
    "print(\"Indices of annotated frames:\", indices_annotated, sep=\"\\n\")\n",
    "rgb_annotated = np.stack([rgb_stack[idx] for idx in indices_annotated])\n",
    "\n",
    "# Take ground truths as annotation\n",
    "seg_annotated = np.stack([seg_stack[idx] for idx in indices_annotated])\n",
    "\n",
    "# Create weights\n",
    "if use_weights:\n",
    "    weights_annotated = np.stack([weights_stack[idx] for idx in indices_annotated])\n",
    "#     weights_annotated = np.zeros_like(seg_annotated)\n",
    "#     for i in range(len(seg_annotated)):\n",
    "#         distances = ndi.distance_transform_edt(1 - seg_annotated[i])\n",
    "#         weights[i] = np.exp(- (distances / 3.0) ** 2) - seg_annotated[i]\n",
    "else:\n",
    "    weights_annotated = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration (over 200):\n",
      "20: dice_annotated = 0.708592 - dice_full = 0.727058\n",
      "40: dice_annotated = 0.766832 - dice_full = 0.741198\n",
      "60: dice_annotated = 0.845639 - dice_full = 0.822925\n",
      "80: dice_annotated = 0.840491 - dice_full = 0.821132\n",
      "100: dice_annotated = 0.783569 - dice_full = 0.775329\n",
      "120: dice_annotated = 0.833248 - dice_full = 0.831159\n",
      "140: dice_annotated = 0.865393 - dice_full = 0.871559\n",
      "160: dice_annotated = 0.894213 - dice_full = 0.916372\n",
      "180: dice_annotated = 0.889699 - dice_full = 0.906463\n",
      "200: dice_annotated = 0.905953 - dice_full = 0.928498\n"
     ]
    }
   ],
   "source": [
    "annotated_per_batch = n_annotated # number of annotated frames in each batch\n",
    "n_iter = 200\n",
    "\n",
    "# Fine tune the model\n",
    "model_ft = copy.deepcopy(model)\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set model to training mode\n",
    "model_ft.train()\n",
    "\n",
    "# Iterate over the data\n",
    "print(\"Iteration (over %d):\" % n_iter)\n",
    "dataloader_iter = iter(dataloaders[\"train\"])\n",
    "for i in range(n_iter):\n",
    "    # Get next batch, and re-initialize dataloader if needed\n",
    "    try:\n",
    "        batch = next(dataloader_iter)\n",
    "    except StopIteration:\n",
    "        dataloader_iter = iter(dataloaders[\"train\"])\n",
    "        batch = next(dataloader_iter)\n",
    "    \n",
    "    ## Replace first elements of the batch by annotations\n",
    "    # Randomly select elements\n",
    "    rand_idx = np.random.choice(np.arange(n_annotated), size=annotated_per_batch, replace=False)\n",
    "    # Keep only relevant input channels\n",
    "    channel_imgs = {\"R\": rgb_annotated[rand_idx,:,:,0],\n",
    "                    \"G\": rgb_annotated[rand_idx,:,:,1],\n",
    "                    \"B\": rgb_annotated[rand_idx,:,:,2]}\n",
    "    images = np.stack([channel_imgs[channel] for channel in input_channels], axis=1)\n",
    "    # Apply train transforms\n",
    "    images = normalize_range(pad_transform_stack(aug_fn(images), u_depth))\n",
    "    targets = pad_transform_stack(seg_annotated[rand_idx], u_depth)\n",
    "    if use_weights:\n",
    "        weights = pad_transform_stack(weights_annotated[rand_idx], u_depth)\n",
    "        items_annotated = [(i, t, w) for i, t, w in zip(images, targets, weights)]\n",
    "    else:\n",
    "        items_annotated = [(i, t) for i, t in zip(images, targets)]\n",
    "    \n",
    "    # Extract items from batch and send to model device\n",
    "    batch[:annotated_per_batch] = items_annotated\n",
    "    batch = collate_fn(batch[:annotated_per_batch * 2])\n",
    "    \n",
    "    batch_x = batch[0].to(model.device)\n",
    "    batch_y = batch[1].to(model.device)\n",
    "    if use_weights: # pixel-wise weights\n",
    "        batch_w = batch[2]\n",
    "        batch_w = batch_w.to(model.device)\n",
    "    else:\n",
    "        batch_w = None\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model_ft(batch_x)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, batch_y, batch_w)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i >= 10 and ((i + 1) % int(n_iter / 10) == 0 or i == 0): \n",
    "        print(\"{}: dice_annotated = {:.6f} - dice_full = {:.6f}\".format(i + 1,\n",
    "            evaluate_stack(model_ft, rgb_stack, seg_stack, batch_size, metrics=metrics,\n",
    "                           input_channels=input_channels,\n",
    "                           transform=lambda stack: normalize_range(pad_transform_stack(stack, u_depth)))[\"dice\"],\n",
    "            evaluate_stack(model_ft, rgb_annotated, seg_annotated, batch_size, metrics=metrics,\n",
    "                           input_channels=input_channels,\n",
    "                           transform=lambda stack: normalize_range(pad_transform_stack(stack, u_depth)))[\"dice\"]))\n",
    "    \n",
    "    if i == n_iter - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526ec34658ca4f6793b3d57d98dfca0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=5), Output()), _dom_classes=('widget-interact',)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(i=(0, len(batch_x) - 1))\n",
    "def plot_batch(i=0):\n",
    "    input = batch_x[i].cpu().numpy()\n",
    "    input = (np.stack([input[0], input[1], input[1]], axis=-1) + 1) / 2\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(input)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(batch_y[i].cpu().numpy())\n",
    "    if use_weights:\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(batch_w[i].cpu().numpy())\n",
    "    plt.tight_layout()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted experiment in 2.3 s.\n",
      "Dice    = 0.19969367975092164\n",
      "Dice_ft = 0.9059531851908033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d72e436cea44a9a8b61a728d9a24f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=567), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict again, and compare results\n",
    "start = time.time()\n",
    "predictions_ft = predict_stack(model_ft, rgb_stack, batch_size, input_channels=input_channels,\n",
    "                               transform=lambda stack: normalize_range(pad_transform_stack(stack, u_depth)))\n",
    "predictions_ft = torch.sigmoid(predictions_ft)\n",
    "print(\"Predicted experiment in %.1f s.\" % (time.time() - start))\n",
    "\n",
    "if seg_stack is not None:\n",
    "    print(\"Dice    =\", dice_coef((predictions > 0.5).numpy(), seg_stack))\n",
    "    print(\"Dice_ft =\", dice_coef((predictions_ft > 0.5).numpy(), seg_stack))\n",
    "\n",
    "@interact(image=(0, len(rgb_stack) - 1))\n",
    "def plot_experiment(image=0):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    if seg_stack is not None:     \n",
    "        plt.subplot(234)\n",
    "        plt.title(\"Binary detection\")\n",
    "        plt.imshow(seg_stack[image], cmap=\"gray\")\n",
    "    plt.subplot(232)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(predictions[image], cmap=\"gray\")\n",
    "    plt.subplot(233)\n",
    "    plt.title(\"Binary prediction\")\n",
    "    plt.imshow((predictions[image] > 0.5), cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(235)\n",
    "    plt.title(\"Fine tuned prediction\")\n",
    "    plt.imshow(predictions_ft[image], cmap=\"gray\")\n",
    "    plt.subplot(236)\n",
    "    plt.title(\"Fine tuned binary prediction\")\n",
    "    plt.imshow((predictions_ft[image] > 0.5), cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
