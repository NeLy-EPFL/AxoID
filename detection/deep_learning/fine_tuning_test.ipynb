{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning test\n",
    "Test the possibility to manually annotated a few frames of the experiment, then fine tune the network on them to predict the rest of the frames.  \n",
    "This is kind of overfitting part of the test set to perform well on the rest, or domain adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time, shutil, copy\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from scipy import ndimage as ndi\n",
    "import cv2\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils_common.image import imread_to_float, to_npint, overlay_preds_targets\n",
    "from utils_common.metrics import dice_coef\n",
    "from utils_data import normalize_range, get_all_dataloaders, pad_transform, pad_transform_stack, compute_weights\n",
    "from utils_loss import get_BCEWithLogits_loss\n",
    "from utils_metric import get_dice_metric\n",
    "from utils_model import CustomUNet, load_model\n",
    "from utils_test import predict, predict_stack, evaluate, evaluate_stack\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed*10 + 1234)\n",
    "torch.manual_seed(seed*100 + 4321)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Choose wether or not use synth, aug, and weights for fine tuning\n",
    "synth_data = False\n",
    "synth_ratio = None # ratio of synthetic data vs. real data\n",
    "only_synth = False # If True, will use only the synthetic data (and all of it, at the opposite of ratio=1)\n",
    "data_aug = False # If True, will use data augmentation (see below for augmentation sequence)\n",
    "use_weights = True # if False use class weights, if True use pixelwise weights (if existing)\n",
    "\n",
    "input_channels = \"RG\" # Channel to use as input\n",
    "u_depth = 4\n",
    "out1_channels = 16\n",
    "\n",
    "out_model_name = \"models/test_ft\"\n",
    "model_name = \"models/unet4-16_RG_cv-annotated/\"\n",
    "data_dir = \"/data/talabot/pdm/dataset_cv-annotated/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training\n",
    "Make dataloaders and so on to prepare fine tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random augment sequence for data augmentation if applicable\n",
    "if data_aug:\n",
    "    seq = iaa.GammaContrast((0.7, 1.3)) # Gamma correction\n",
    "    aug_fn = seq.augment_image\n",
    "else:\n",
    "    aug_fn = lambda x: x # identity function\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = get_all_dataloaders(\n",
    "    data_dir,\n",
    "    batch_size, \n",
    "    input_channels = input_channels,\n",
    "    test_dataloader = True,\n",
    "    use_weights = use_weights,\n",
    "    synthetic_data = synth_data, synthetic_ratio = synth_ratio, synthetic_only = only_synth,\n",
    "    train_transform = lambda img: normalize_range(pad_transform(aug_fn(img), u_depth)),\n",
    "    train_target_transform = lambda img: pad_transform(img, u_depth),\n",
    "    eval_transform = lambda img: normalize_range(pad_transform(img, u_depth)), \n",
    "    eval_target_transform = lambda img: pad_transform(img, u_depth)\n",
    ")\n",
    "# \"Deactivate\" the collate_fn of the train dataloader\n",
    "collate_fn = dataloaders[\"train\"].collate_fn\n",
    "dataloaders[\"train\"].collate_fn = lambda batch: batch\n",
    "\n",
    "# Compute class weights (as pixel imbalance)\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "for filename in dataloaders[\"train\"].dataset.y_filenames:\n",
    "    y = io.imread(filename)\n",
    "    pos_count += (y == 255).sum()\n",
    "    neg_count += (y == 0).sum()\n",
    "pos_weight = torch.tensor((neg_count + pos_count) / (2 * pos_count)).to(device)\n",
    "neg_weight = torch.tensor((neg_count + pos_count) / (2 * neg_count)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/test_ft/utils_model_save.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(model_name, input_channels=input_channels, u_depth=u_depth, \n",
    "                   out1_channels=out1_channels, device=device)\n",
    "\n",
    "loss_fn = get_BCEWithLogits_loss(pos_weight=pos_weight, neg_weight=neg_weight)\n",
    "metrics = {\"dice\": get_dice_metric()}\n",
    "\n",
    "# Save future model\n",
    "os.makedirs(out_model_name, exist_ok=True)\n",
    "shutil.copy(\"utils_model.py\", os.path.join(out_model_name, \"utils_model_save.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load experiments and annotated frames\n",
    "Load an experiment, predict once the detections, and create annotations for a few frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted experiment in 2.3 s.\n",
      "Dice = 0.19969367975092164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baeedee0d4148b590f9a42254246621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=567), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = \"/data/talabot/experiments/annotated/R70H06_20181202-tdTomGC6fopt-fl2/R70H06-tdTomGC6fopt-fly2-001/\"\n",
    "\n",
    "# Load experiment and segmentation & weights if available\n",
    "rgb_stack = imread_to_float(os.path.join(experiment, \"RGB.tif\"))\n",
    "if os.path.isfile(os.path.join(experiment, \"seg_ROI.tif\")):\n",
    "    seg_stack = imread_to_float(os.path.join(experiment, \"seg_ROI.tif\"))\n",
    "else:\n",
    "    seg_stack = None\n",
    "if use_weights and os.path.isfile(os.path.join(experiment, \"weights.tif\")):\n",
    "    weights_stack = imread_to_float(os.path.join(experiment, \"weights.tif\"))\n",
    "else:\n",
    "    weights_stack = None\n",
    "\n",
    "# Predict using loaded model\n",
    "start = time.time()\n",
    "predictions = predict_stack(model, rgb_stack, batch_size, input_channels=input_channels,\n",
    "                            transform=lambda stack: normalize_range(pad_transform_stack(stack, u_depth)))\n",
    "predictions = torch.sigmoid(predictions)\n",
    "print(\"Predicted experiment in %.1f s.\" % (time.time() - start))\n",
    "\n",
    "if seg_stack is not None:\n",
    "    print(\"Dice =\", dice_coef((predictions > 0.5).numpy(), seg_stack))\n",
    "\n",
    "@interact(image=(0, len(rgb_stack) - 1))\n",
    "def plot_experiment(image=0):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    if seg_stack is not None:\n",
    "        plt.subplot(232)\n",
    "        plt.title(\"Binary detection\")\n",
    "        plt.imshow(seg_stack[image], cmap=\"gray\")\n",
    "    if weights_stack is not None:\n",
    "        plt.subplot(233)\n",
    "        plt.title(\"Pixel weighting\")\n",
    "        plt.imshow(weights_stack[image], cmap=\"gray\")\n",
    "    plt.subplot(235)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(predictions[image], cmap=\"gray\")\n",
    "    if seg_stack is not None:\n",
    "        plt.subplot(236)\n",
    "        plt.title(\"Overlay with ground truth\")\n",
    "        plt.imshow(overlay_preds_targets((predictions[image] > 0.5).numpy(), seg_stack[image]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ground truth as annotations to test how many frames are needed, and how to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of annotated frames:\n",
      "[436 289 480 135]\n",
      "96.120148 positive and 0.502614 negative weighting.\n"
     ]
    }
   ],
   "source": [
    "# Select and annotated frames\n",
    "n_annotations = 4 # number of frames to annotated\n",
    "n_valid = 1 # number of annotated frames to use for validation\n",
    "\n",
    "# Randomly choose frames\n",
    "indices_annotated = np.random.choice(np.arange(len(rgb_stack)), size=n_annotations, replace=False)\n",
    "print(\"Indices of annotated frames:\", indices_annotated, sep=\"\\n\")\n",
    "rgb_annotated = np.stack([rgb_stack[idx] for idx in indices_annotated])\n",
    "\n",
    "# Take ground truths as annotation\n",
    "seg_annotated = np.stack([seg_stack[idx] for idx in indices_annotated])\n",
    "\n",
    "# Create weights\n",
    "if use_weights:\n",
    "    weights_annotated = np.stack([weights_stack[idx] for idx in indices_annotated])\n",
    "#     weights_annotated = compute_weights(seg_annotated)\n",
    "else:\n",
    "    weights_annotated = None\n",
    "\n",
    "# Compute class weights (as pixel imbalance)\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "# for filename in dataloaders[\"train\"].dataset.y_filenames:\n",
    "#     y = io.imread(filename)\n",
    "#     pos_count += (y == 255).sum()\n",
    "#     neg_count += (y == 0).sum()\n",
    "pos_count += (seg_annotated[:n_annotations - n_valid] == 1).sum() #* (len(dataloaders[\"train\"].dataset) // n_annotations)\n",
    "neg_count += (seg_annotated[:n_annotations - n_valid] == 0).sum() #* (len(dataloaders[\"train\"].dataset) // n_annotations)\n",
    "pos_weight = torch.tensor((neg_count + pos_count) / (2 * pos_count)).to(device)\n",
    "neg_weight = torch.tensor((neg_count + pos_count) / (2 * neg_count)).to(device)\n",
    "print(\"{:.6f} positive and {:.6f} negative weighting.\".format(pos_weight.item(), neg_weight.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration (over 200):\n",
      "20: dice_annotated = 0.846047 - dice_valid = 0.846229 - dice_full = 0.851170\n",
      "40: dice_annotated = 0.871750 - dice_valid = 0.851064 - dice_full = 0.863276\n",
      "60: dice_annotated = 0.895420 - dice_valid = 0.882834 - dice_full = 0.877214\n",
      "80: dice_annotated = 0.910595 - dice_valid = 0.889088 - dice_full = 0.887204\n",
      "100: dice_annotated = 0.924317 - dice_valid = 0.897297 - dice_full = 0.893143\n",
      "120: dice_annotated = 0.941968 - dice_valid = 0.901357 - dice_full = 0.897816\n",
      "140: dice_annotated = 0.953773 - dice_valid = 0.904632 - dice_full = 0.900697\n",
      "160: dice_annotated = 0.964700 - dice_valid = 0.906278 - dice_full = 0.904454\n",
      "180: dice_annotated = 0.975496 - dice_valid = 0.916895 - dice_full = 0.909051\n",
      "200: dice_annotated = 0.980084 - dice_valid = 0.922374 - dice_full = 0.912047\n",
      "Best model fine tuned in iteration 193.\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "annotated_per_batch = min(n_annotations - n_valid, batch_size) # number of annotated frames in each batch\n",
    "n_iter = 200\n",
    "\n",
    "# Fine tune the model\n",
    "model_ft = copy.deepcopy(model)\n",
    "loss_fn = get_BCEWithLogits_loss(pos_weight=pos_weight, neg_weight=neg_weight)\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set model to training mode\n",
    "model_ft.train()\n",
    "\n",
    "# Iterate over the data\n",
    "print(\"Iteration (over %d):\" % n_iter)\n",
    "# dataloader_iter = iter(dataloaders[\"train\"])\n",
    "best_iter, best_dice = -1, 0\n",
    "for i in range(n_iter):\n",
    "    # Get next batch, and re-initialize dataloader if needed\n",
    "#     try:\n",
    "#         batch = next(dataloader_iter)\n",
    "#     except StopIteration:\n",
    "#         dataloader_iter = iter(dataloaders[\"train\"])\n",
    "#         batch = next(dataloader_iter)\n",
    "    \n",
    "    ## Replace first elements of the batch by annotations\n",
    "    # Randomly select elements\n",
    "    rand_idx = np.random.choice(np.arange(n_annotations - n_valid), size=annotated_per_batch, replace=False)\n",
    "    # Keep only relevant input channels\n",
    "    channel_imgs = {\"R\": rgb_annotated[rand_idx,:,:,0],\n",
    "                    \"G\": rgb_annotated[rand_idx,:,:,1],\n",
    "                    \"B\": rgb_annotated[rand_idx,:,:,2]}\n",
    "    images = np.stack([channel_imgs[channel] for channel in input_channels], axis=1)\n",
    "    # Apply train transforms\n",
    "    images = [normalize_range(pad_transform(aug_fn(image), u_depth)) for image in images]\n",
    "    targets = pad_transform_stack(seg_annotated[rand_idx], u_depth)\n",
    "    if use_weights:\n",
    "        weights = pad_transform_stack(weights_annotated[rand_idx], u_depth)\n",
    "        items_annotated = [(i, t, w) for i, t, w in zip(images, targets, weights)]\n",
    "    else:\n",
    "        items_annotated = [(i, t) for i, t in zip(images, targets)]\n",
    "    \n",
    "    # Extract items from batch and send to model device\n",
    "#     batch[:annotated_per_batch] = items_annotated\n",
    "#     batch = collate_fn(batch[:annotated_per_batch * 2])\n",
    "    batch = collate_fn(items_annotated)\n",
    "    \n",
    "    batch_x = batch[0].to(model.device)\n",
    "    batch_y = batch[1].to(model.device)\n",
    "    if use_weights: # pixel-wise weights\n",
    "        batch_w = batch[2]\n",
    "        batch_w = batch_w.to(model.device)\n",
    "    else:\n",
    "        batch_w = None\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model_ft(batch_x)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, batch_y, batch_w)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if n_valid > 0:\n",
    "        valid_dice = evaluate_stack(model_ft, rgb_annotated[n_annotations - n_valid:],\n",
    "                                    seg_annotated[n_annotations - n_valid:], batch_size, metrics=metrics,\n",
    "                                    input_channels=input_channels,\n",
    "                                    transform=lambda stack: normalize_range(\n",
    "                                        pad_transform_stack(stack, u_depth)))[\"dice\"]\n",
    "        if best_dice < valid_dice:\n",
    "            best_iter = i\n",
    "            best_dice = valid_dice\n",
    "            torch.save(model_ft.state_dict(), os.path.join(out_model_name, \"model_best.pth\"))\n",
    "    else:\n",
    "        valid_dice = 0.0\n",
    "    \n",
    "    if n_iter >= 10 and (i + 1) % int(n_iter / 10) == 0:\n",
    "        print(\"{}: dice_annotated = {:.6f} - dice_valid = {:.6f} - dice_full = {:.6f}\".format(i + 1,\n",
    "            evaluate_stack(model_ft, rgb_annotated[:n_annotations - n_valid], \n",
    "                           seg_annotated[:n_annotations - n_valid], batch_size, metrics=metrics,\n",
    "                           input_channels=input_channels,\n",
    "                           transform=lambda stack: normalize_range(pad_transform_stack(stack, u_depth)))[\"dice\"],\n",
    "            valid_dice,\n",
    "            evaluate_stack(model_ft, rgb_stack, seg_stack, batch_size, metrics=metrics,\n",
    "                           input_channels=input_channels,\n",
    "                           transform=lambda stack: normalize_range(pad_transform_stack(stack, u_depth)))[\"dice\"]))\n",
    "# Load best model found\n",
    "if n_valid > 0:\n",
    "    print(\"Best model fine tuned in iteration %d.\" % best_iter)\n",
    "    model_ft.load_state_dict(torch.load(os.path.join(out_model_name, \"model_best.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd0f052537d4408982331268e473618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(i=(0, len(batch_x) - 1))\n",
    "def plot_batch(i=0):\n",
    "    input = batch_x[i].cpu().numpy()\n",
    "    input = (np.stack([input[0], input[1], input[1]], axis=-1) + 1) / 2\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(input)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(batch_y[i].cpu().numpy())\n",
    "    if use_weights:\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(batch_w[i].cpu().numpy())\n",
    "    plt.tight_layout()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted experiment in 2.2 s.\n",
      "Dice    = 0.19969367975092164\n",
      "Dice_ft = 0.9111517111723493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8ad61929444199b2706ed8a1dde4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image', max=567), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict again, and compare results\n",
    "start = time.time()\n",
    "predictions_ft = predict_stack(model_ft, rgb_stack, batch_size, input_channels=input_channels,\n",
    "                               transform=lambda stack: normalize_range(pad_transform_stack(stack, u_depth)))\n",
    "predictions_ft = torch.sigmoid(predictions_ft)\n",
    "print(\"Predicted experiment in %.1f s.\" % (time.time() - start))\n",
    "\n",
    "if seg_stack is not None:\n",
    "    print(\"Dice    =\", dice_coef((predictions > 0.5).numpy(), seg_stack))\n",
    "    print(\"Dice_ft =\", dice_coef((predictions_ft > 0.5).numpy(), seg_stack))\n",
    "\n",
    "@interact(image=(0, len(rgb_stack) - 1))\n",
    "def plot_experiment(image=0):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title(\"Raw input\")\n",
    "    plt.imshow(rgb_stack[image])\n",
    "    if seg_stack is not None:     \n",
    "        plt.subplot(234)\n",
    "        plt.title(\"Binary detection\")\n",
    "        plt.imshow(seg_stack[image], cmap=\"gray\")\n",
    "    plt.subplot(232)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(predictions[image], cmap=\"gray\")\n",
    "    if seg_stack is not None:\n",
    "        plt.subplot(233)\n",
    "        plt.title(\"Overlay with ground truth\")\n",
    "        plt.imshow(overlay_preds_targets((predictions[image] > 0.5).numpy(), seg_stack[image]))\n",
    "    plt.subplot(235)\n",
    "    plt.title(\"Fine tuned prediction\")\n",
    "    plt.imshow(predictions_ft[image], cmap=\"gray\")\n",
    "    if seg_stack is not None:\n",
    "        plt.subplot(236)\n",
    "        plt.title(\"Overlay with ground truth\")\n",
    "        plt.imshow(overlay_preds_targets((predictions_ft[image] > 0.5).numpy(), seg_stack[image]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import imageio as io\n",
    "\n",
    "N = 100\n",
    "\n",
    "io.mimsave(\"/home/user/talabot/workdir/input.gif\", to_npint(rgb_stack[:N]), duration=1)\n",
    "io.mimsave(\"/home/user/talabot/workdir/preds.gif\", to_npint(predictions[:N].numpy() > 0.5), duration=1)\n",
    "io.mimsave(\"/home/user/talabot/workdir/preds_ft.gif\", to_npint(predictions_ft[:N].numpy() > 0.5), duration=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test drawing for manual annotations\n",
    "Test drawing by mouse."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from utils_finetuning import ROISelector\n",
    "\n",
    "img = io.imread(\"/data/talabot/experiments/annotated/R70H06_20181202-tdTomGC6fopt-fl2/\"\n",
    "                \"R70H06-tdTomGC6fopt-fly2-001/rgb_frames/rgb_0000.png\")\n",
    "\n",
    "roi_selector = ROISelector(img)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(roi_selector.segmentation, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
